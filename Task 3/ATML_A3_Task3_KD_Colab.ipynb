{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# ATML Assignment 3 \u2014 Task 3: Knowledge Distillation (Colab, T4 GPU)\n", "\n", "Lean, single-notebook pipeline for CIFAR-100 with VGG-16/19 (teacher) and VGG-11 (student).\n", "\n", "**Experiments covered**\n", "1. Independent Student (SI)\n", "2. Logit Matching (LM)\n", "3. Label Smoothing (LS) baseline\n", "4. Decoupled KD (DKD)\n", "5. Hints KD (intermediate feature matching)\n", "6. Contrastive Representation Distillation (CRD)\n", "7. Color-invariance transfer (CRD with jittered teacher)\n", "8. Larger teacher check (VGG-16 vs VGG-19 \u2192 LM)\n", "\n", "**Artifacts saved** (CSV/PNG/CKPT):\n", "- `ckpts/teacher_vgg16.pt`, `teacher_vgg19.pt`, `student_*.pt`\n", "- `results/metrics_overall.csv`, `results/kl_alignment.csv`, `results/color_invariance.csv`\n", "- `figures/acc_bar.png`, `figures/kl_bar.png`, `figures/gradcam_grid.png`, `figures/color_shift.png`\n", "- `gradcam/IMGxxxx_METHOD.png`\n", "\n", "**Notes**\n", "- Keep epochs modest on Colab T4. Use AMP. Fix seeds.\n", "- For Grad-CAM, we use `pytorch-grad-cam` if available; otherwise, skip gracefully.\n"]}, {"cell_type": "code", "metadata": {"id": "setup_installs"}, "execution_count": null, "outputs": [], "source": ["# %%capture\n", "# If running in Colab, uncomment installs as needed.\n", "# !pip -q install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n", "# !pip -q install pytorch-grad-cam==1.4.8\n", "# !pip -q install pandas matplotlib scikit-learn\n", "print(\"If running in Colab, enable GPU (T4) and uncomment installs above if needed.\")"]}, {"cell_type": "code", "metadata": {"id": "imports_and_paths"}, "execution_count": null, "outputs": [], "source": ["import os, random, json, math, time\n", "from pathlib import Path\n", "import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import DataLoader\n", "from torchvision import datasets, transforms, models\n", "from datetime import datetime\n", "\n", "USE_AMP = True\n", "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "\n", "ROOT = Path('/content/ATML_A3') if 'google.colab' in str(getattr(sys.modules.get('google'), '__file__', '')) else Path.cwd() / 'ATML_A3'\n", "CKPTS = ROOT / 'ckpts'\n", "FIGS  = ROOT / 'figures'\n", "GRADS = ROOT / 'gradcam'\n", "LOGS  = ROOT / 'logs'\n", "RES   = ROOT / 'results'\n", "for p in [CKPTS, FIGS, GRADS, LOGS, RES]:\n", "    p.mkdir(parents=True, exist_ok=True)\n", "print(f\"Project root: {ROOT}\")\n", "print(f\"Device: {DEVICE}\")"]}, {"cell_type": "code", "metadata": {"id": "seeding_utils"}, "execution_count": null, "outputs": [], "source": ["def set_seed(seed=1337):\n", "    random.seed(seed)\n", "    np.random.seed(seed)\n", "    torch.manual_seed(seed)\n", "    torch.cuda.manual_seed_all(seed)\n", "    torch.backends.cudnn.deterministic = True\n", "    torch.backends.cudnn.benchmark = False\n", "\n", "set_seed(1337)\n", "print(\"Seeds fixed.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Data: CIFAR-100 (train/val loaders)"]}, {"cell_type": "code", "metadata": {"id": "data_loaders"}, "execution_count": null, "outputs": [], "source": ["def get_cifar100(batch_size=128, num_workers=2, color_jitter=False):\n", "    mean = (0.5071, 0.4867, 0.4408)\n", "    std  = (0.2675, 0.2565, 0.2761)\n", "    aug = [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()]\n", "    if color_jitter:\n", "        aug.append(transforms.ColorJitter(0.4,0.4,0.4,0.1))\n", "    train_tf = transforms.Compose(aug + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n", "    test_tf  = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n", "    train_ds = datasets.CIFAR100(root=str(ROOT/'data'), train=True,  download=True, transform=train_tf)\n", "    test_ds  = datasets.CIFAR100(root=str(ROOT/'data'), train=False, download=True, transform=test_tf)\n", "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n", "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n", "    return train_loader, test_loader\n", "\n", "train_loader, val_loader = get_cifar100()\n", "print(\"Data loaders ready.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Models: VGG-16/19 (Teacher), VGG-11 (Student)"]}, {"cell_type": "code", "metadata": {"id": "models"}, "execution_count": null, "outputs": [], "source": ["def make_vgg(name='vgg16', num_classes=100, pretrained=False):\n", "    if name == 'vgg16':\n", "        net = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1 if pretrained else None)\n", "    elif name == 'vgg19':\n", "        net = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1 if pretrained else None)\n", "    elif name == 'vgg11':\n", "        net = models.vgg11(weights=None)\n", "    else:\n", "        raise ValueError('Unsupported VGG: ' + name)\n", "    # replace classifier for CIFAR-100\n", "    in_feats = net.classifier[-1].in_features\n", "    net.classifier[-1] = nn.Linear(in_feats, num_classes)\n", "    return net\n", "\n", "teacher16 = make_vgg('vgg16', pretrained=True).to(DEVICE)\n", "student11 = make_vgg('vgg11', pretrained=False).to(DEVICE)\n", "print(\"Teacher VGG-16 & Student VGG-11 created.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Losses: CE, Label Smoothing, KD (LM), DKD (stub), Hints, CRD (minimal)"]}, {"cell_type": "code", "metadata": {"id": "losses"}, "execution_count": null, "outputs": [], "source": ["class LabelSmoothingCE(nn.Module):\n", "    def __init__(self, eps=0.1):\n", "        super().__init__()\n", "        self.eps = eps\n", "        self.log_softmax = nn.LogSoftmax(dim=1)\n", "    def forward(self, logits, targets):\n", "        n_classes = logits.size(1)\n", "        logprobs = self.log_softmax(logits)\n", "        with torch.no_grad():\n", "            true_dist = torch.zeros_like(logprobs)\n", "            true_dist.fill_(self.eps / (n_classes - 1))\n", "            true_dist.scatter_(1, targets.unsqueeze(1), 1 - self.eps)\n", "        return torch.mean(torch.sum(-true_dist * logprobs, dim=1))\n", "\n", "def kd_loss_logits(student_logits, teacher_logits, T=4.0):\n", "    # KL(T||S) with temperature scaling (standard LM)\n", "    p = F.log_softmax(student_logits / T, dim=1)\n", "    q = F.softmax(teacher_logits / T, dim=1)\n", "    return F.kl_div(p, q, reduction='batchmean') * (T * T)\n", "\n", "class DKDLoss(nn.Module):\n", "    # Minimal DKD stub \u2014 you may refine alpha/beta schedules as needed\n", "    def __init__(self, alpha=1.0, beta=8.0, T=4.0):\n", "        super().__init__()\n", "        self.alpha=alpha; self.beta=beta; self.T=T\n", "    def forward(self, s_logits, t_logits, targets):\n", "        # Decouple target vs non-target components (very compact form)\n", "        T = self.T\n", "        s = F.log_softmax(s_logits/T, dim=1)\n", "        t = F.softmax(t_logits/T, dim=1)\n", "        # target class\n", "        one_hot = F.one_hot(targets, num_classes=s_logits.size(1)).float()\n", "        pos_loss = F.kl_div((s*one_hot).sum(1, keepdim=True), (t*one_hot).sum(1, keepdim=True), reduction='batchmean')\n", "        # non-target classes\n", "        neg_loss = F.kl_div((s*(1-one_hot)), (t*(1-one_hot)), reduction='batchmean')\n", "        return (self.alpha*pos_loss + self.beta*neg_loss) * (T*T)\n", "\n", "class HintLoss(nn.Module):\n", "    def __init__(self, proj_s: nn.Module, weight=1e-2):\n", "        super().__init__()\n", "        self.proj_s = proj_s\n", "        self.weight = weight\n", "        self.mse = nn.MSELoss()\n", "    def forward(self, feat_s, feat_t):\n", "        return self.weight * self.mse(self.proj_s(feat_s), feat_t)\n", "\n", "class CRDLoss(nn.Module):\n", "    # Lightweight contrastive loss over batch (no memory bank for simplicity)\n", "    def __init__(self, dim=128, T=0.07):\n", "        super().__init__()\n", "        self.T=T\n", "        self.proj_t = nn.Linear(dim, dim, bias=False)\n", "        self.proj_s = nn.Linear(dim, dim, bias=False)\n", "    def forward(self, z_s, z_t):\n", "        z_s = F.normalize(self.proj_s(z_s), dim=1)\n", "        z_t = F.normalize(self.proj_t(z_t), dim=1)\n", "        logits = (z_s @ z_t.t()) / self.T\n", "        labels = torch.arange(z_s.size(0), device=z_s.device)\n", "        return F.cross_entropy(logits, labels)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train/Eval helpers"]}, {"cell_type": "code", "metadata": {"id": "train_helpers"}, "execution_count": null, "outputs": [], "source": ["def accuracy_topk(logits, targets, topk=(1,)):\n", "    maxk = max(topk)\n", "    batch_size = targets.size(0)\n", "    _, pred = logits.topk(maxk, 1, True, True)\n", "    pred = pred.t()\n", "    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n", "    res = []\n", "    for k in topk:\n", "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n", "        res.append(correct_k.mul_(100.0 / batch_size))\n", "    return res\n", "\n", "def evaluate(model, loader):\n", "    model.eval()\n", "    loss_sum=0.0; n=0; top1=0.0; top5=0.0\n", "    ce = nn.CrossEntropyLoss()\n", "    with torch.no_grad():\n", "        for x,y in loader:\n", "            x=x.to(DEVICE); y=y.to(DEVICE)\n", "            logits = model(x)\n", "            loss = ce(logits, y)\n", "            a1,a5 = accuracy_topk(logits, y, topk=(1,5))\n", "            bs = x.size(0)\n", "            loss_sum += loss.item()*bs\n", "            n += bs\n", "            top1 += a1.item()*bs/100.0\n", "            top5 += a5.item()*bs/100.0\n", "    return loss_sum/n, 100*top1/n, 100*top5/n\n", "\n", "def train_ce(model, train_loader, val_loader, epochs=60, lr=0.1, weight_decay=5e-4):\n", "    model.to(DEVICE)\n", "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n", "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n", "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n", "    ce = nn.CrossEntropyLoss()\n", "    best=(1e9,0,0)\n", "    for ep in range(1, epochs+1):\n", "        model.train()\n", "        for x,y in train_loader:\n", "            x=x.to(DEVICE); y=y.to(DEVICE)\n", "            opt.zero_grad(set_to_none=True)\n", "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n", "                logits = model(x)\n", "                loss = ce(logits, y)\n", "            scaler.scale(loss).backward()\n", "            scaler.step(opt)\n", "            scaler.update()\n", "        sched.step()\n", "        vl, a1, a5 = evaluate(model, val_loader)\n", "        if a1>best[1]: best=(vl,a1,a5)\n", "        if ep%10==0 or ep==1:\n", "            print(f\"[CE] Epoch {ep}/{epochs} | val_loss={vl:.3f} top1={a1:.2f} top5={a5:.2f}\")\n", "    return best\n", "\n", "def train_kd_lm(student, teacher, train_loader, val_loader, epochs=60, lr=0.1, alpha=0.5, T=4.0):\n", "    student.to(DEVICE)\n", "    teacher.eval().to(DEVICE)\n", "    opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n", "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n", "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n", "    ce = nn.CrossEntropyLoss()\n", "    best=(1e9,0,0)\n", "    for ep in range(1, epochs+1):\n", "        student.train()\n", "        for x,y in train_loader:\n", "            x=x.to(DEVICE); y=y.to(DEVICE)\n", "            with torch.no_grad():\n", "                t_logits = teacher(x)\n", "            opt.zero_grad(set_to_none=True)\n", "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n", "                s_logits = student(x)\n", "                loss = alpha*kd_loss_logits(s_logits, t_logits, T=T) + (1-alpha)*ce(s_logits, y)\n", "            scaler.scale(loss).backward()\n", "            scaler.step(opt)\n", "            scaler.update()\n", "        sched.step()\n", "        vl, a1, a5 = evaluate(student, val_loader)\n", "        if a1>best[1]: best=(vl,a1,a5)\n", "        if ep%10==0 or ep==1:\n", "            print(f\"[KD-LM] Epoch {ep}/{epochs} | val_loss={vl:.3f} top1={a1:.2f} top5={a5:.2f}\")\n", "    return best\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Probability alignment (mean KL(T\u2225S))"]}, {"cell_type": "code", "metadata": {"id": "prob_alignment"}, "execution_count": null, "outputs": [], "source": ["def mean_kl_teacher_student(teacher, student, loader, T=4.0, max_batches=50):\n", "    teacher.eval(); student.eval()\n", "    kl_sum=0.0; n=0\n", "    with torch.no_grad():\n", "        for b,(x,y) in enumerate(loader):\n", "            if b>=max_batches: break\n", "            x=x.to(DEVICE)\n", "            t_logits = teacher(x)\n", "            s_logits = student(x)\n", "            kl = kd_loss_logits(s_logits, t_logits, T=T).item()\n", "            kl_sum += kl\n", "            n += 1\n", "    return kl_sum/max(1,n)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Hints KD (feature hooks)"]}, {"cell_type": "code", "metadata": {"id": "hints_kd"}, "execution_count": null, "outputs": [], "source": ["def _get_conv_feature_module_vgg(model, stage_idx=3):\n", "    # Grab the stage module (rough heuristic: features block slice)\n", "    return model.features\n", "\n", "def collect_features(module, input, output, storage: dict, key: str):\n", "    storage[key] = output\n", "\n", "def train_kd_hints(student, teacher, train_loader, val_loader, epochs=60, lr=0.1, hint_weight=1e-2):\n", "    student.to(DEVICE); teacher.to(DEVICE).eval()\n", "    # register simple hooks on the tail of features\n", "    feats_s, feats_t = {}, {}\n", "    h_s = _get_conv_feature_module_vgg(student).register_forward_hook(lambda m,i,o: collect_features(m,i,o,feats_s,'s'))\n", "    h_t = _get_conv_feature_module_vgg(teacher).register_forward_hook(lambda m,i,o: collect_features(m,i,o,feats_t,'t'))\n", "    # projection 1x1 (conv) to match channels if needed\n", "    proj = nn.Conv2d(student.features[-1].out_channels if hasattr(student.features[-1],'out_channels') else 512, 512, kernel_size=1).to(DEVICE)\n", "    hint_crit = HintLoss(proj, weight=hint_weight)\n", "    ce = nn.CrossEntropyLoss()\n", "    opt = torch.optim.SGD(list(student.parameters())+list(proj.parameters()), lr=lr, momentum=0.9, weight_decay=5e-4)\n", "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n", "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n", "    best=(1e9,0,0)\n", "    for ep in range(1, epochs+1):\n", "        student.train()\n", "        for x,y in train_loader:\n", "            x=x.to(DEVICE); y=y.to(DEVICE)\n", "            with torch.no_grad():\n", "                _ = teacher(x)\n", "                feat_t = feats_t.get('t')\n", "            opt.zero_grad(set_to_none=True)\n", "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n", "                logits = student(x)\n", "                feat_s = feats_s.get('s')\n", "                h_loss = hint_crit(feat_s, feat_t)\n", "                loss = ce(logits, y) + h_loss\n", "            scaler.scale(loss).backward()\n", "            scaler.step(opt)\n", "            scaler.update()\n", "        sched.step()\n", "        vl, a1, a5 = evaluate(student, val_loader)\n", "        if a1>best[1]: best=(vl,a1,a5)\n", "        if ep%10==0 or ep==1:\n", "            print(f\"[Hints] Epoch {ep}/{epochs} | val_loss={vl:.3f} top1={a1:.2f} top5={a5:.2f}\")\n", "    h_s.remove(); h_t.remove()\n", "    return best\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## CRD (batch-only minimal contrastive)"]}, {"cell_type": "code", "metadata": {"id": "crd_kd"}, "execution_count": null, "outputs": [], "source": ["class GlobalAvgPool(nn.Module):\n", "    def forward(self, x):\n", "        return F.adaptive_avg_pool2d(x, (1,1)).flatten(1)\n", "\n", "def train_kd_crd(student, teacher, train_loader, val_loader, epochs=60, lr=0.1, rep_dim=128):\n", "    student.to(DEVICE); teacher.to(DEVICE).eval()\n", "    # Extract penultimate features via simple head (GAP over last conv)\n", "    gap = GlobalAvgPool()\n", "    crd = CRDLoss(dim=512 if 'vgg' in teacher.__class__.__name__.lower() else rep_dim)\n", "    ce = nn.CrossEntropyLoss()\n", "    opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n", "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n", "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n", "    best=(1e9,0,0)\n", "    for ep in range(1, epochs+1):\n", "        student.train()\n", "        for x,y in train_loader:\n", "            x=x.to(DEVICE); y=y.to(DEVICE)\n", "            with torch.no_grad():\n", "                t_logits = teacher(x)\n", "                t_feat = gap(teacher.features(x)) if hasattr(teacher,'features') else t_logits\n", "            opt.zero_grad(set_to_none=True)\n", "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n", "                s_logits = student(x)\n", "                s_feat = gap(student.features(x)) if hasattr(student,'features') else s_logits\n", "                loss = ce(s_logits, y) + crd(s_feat, t_feat)\n", "            scaler.scale(loss).backward()\n", "            scaler.step(opt)\n", "            scaler.update()\n", "        sched.step()\n", "        vl, a1, a5 = evaluate(student, val_loader)\n", "        if a1>best[1]: best=(vl,a1,a5)\n", "        if ep%10==0 or ep==1:\n", "            print(f\"[CRD] Epoch {ep}/{epochs} | val_loss={vl:.3f} top1={a1:.2f} top5={a5:.2f}\")\n", "    return best\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training scripts: Teacher, SI, KD variants"]}, {"cell_type": "code", "metadata": {"id": "train_scripts"}, "execution_count": null, "outputs": [], "source": ["def save_ckpt(model, path):\n", "    torch.save({'state_dict': model.state_dict()}, path)\n", "\n", "def load_ckpt(model, path):\n", "    sd = torch.load(path, map_location=DEVICE)\n", "    model.load_state_dict(sd['state_dict'])\n", "\n", "def train_teacher_vgg16(epochs=60):\n", "    model = make_vgg('vgg16', pretrained=True).to(DEVICE)\n", "    best = train_ce(model, train_loader, val_loader, epochs=epochs, lr=0.1)\n", "    save_ckpt(model, str(CKPTS/'teacher_vgg16.pt'))\n", "    print(\"Saved teacher_vgg16.pt | best:\", best)\n", "    return model\n", "\n", "def train_teacher_vgg19(epochs=60):\n", "    model = make_vgg('vgg19', pretrained=True).to(DEVICE)\n", "    best = train_ce(model, train_loader, val_loader, epochs=epochs, lr=0.1)\n", "    save_ckpt(model, str(CKPTS/'teacher_vgg19.pt'))\n", "    print(\"Saved teacher_vgg19.pt | best:\", best)\n", "    return model\n", "\n", "def train_student_si(epochs=60):\n", "    model = make_vgg('vgg11', pretrained=False).to(DEVICE)\n", "    best = train_ce(model, train_loader, val_loader, epochs=epochs, lr=0.1)\n", "    save_ckpt(model, str(CKPTS/'student_SI.pt'))\n", "    print(\"Saved student_SI.pt | best:\", best)\n", "    return model\n", "\n", "def distill_lm(teacher_path=None, epochs=60, alpha=0.5, T=4.0):\n", "    t = make_vgg('vgg16', pretrained=False).to(DEVICE)\n", "    if teacher_path is None: teacher_path = CKPTS/'teacher_vgg16.pt'\n", "    load_ckpt(t, str(teacher_path))\n", "    s = make_vgg('vgg11', pretrained=False).to(DEVICE)\n", "    best = train_kd_lm(s, t, train_loader, val_loader, epochs=epochs, lr=0.1, alpha=alpha, T=T)\n", "    save_ckpt(s, str(CKPTS/'student_LM.pt'))\n", "    print(\"Saved student_LM.pt | best:\", best)\n", "    return s\n", "\n", "def distill_hints(teacher_path=None, epochs=60):\n", "    t = make_vgg('vgg16', pretrained=False).to(DEVICE)\n", "    if teacher_path is None: teacher_path = CKPTS/'teacher_vgg16.pt'\n", "    load_ckpt(t, str(teacher_path))\n", "    s = make_vgg('vgg11', pretrained=False).to(DEVICE)\n", "    best = train_kd_hints(s, t, train_loader, val_loader, epochs=epochs, lr=0.1)\n", "    save_ckpt(s, str(CKPTS/'student_HINTS.pt'))\n", "    print(\"Saved student_HINTS.pt | best:\", best)\n", "    return s\n", "\n", "def distill_crd(teacher_path=None, epochs=60):\n", "    t = make_vgg('vgg16', pretrained=False).to(DEVICE)\n", "    if teacher_path is None: teacher_path = CKPTS/'teacher_vgg16.pt'\n", "    load_ckpt(t, str(teacher_path))\n", "    s = make_vgg('vgg11', pretrained=False).to(DEVICE)\n", "    best = train_kd_crd(s, t, train_loader, val_loader, epochs=epochs, lr=0.1)\n", "    save_ckpt(s, str(CKPTS/'student_CRD.pt'))\n", "    print(\"Saved student_CRD.pt | best:\", best)\n", "    return s\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## KL alignment, Color-invariance, Larger teacher"]}, {"cell_type": "code", "metadata": {"id": "extras_alignment_color"}, "execution_count": null, "outputs": [], "source": ["import pandas as pd\n", "\n", "def kl_alignment_table(teacher_path, student_paths, names, T=4.0):\n", "    t = make_vgg('vgg16', pretrained=False).to(DEVICE)\n", "    load_ckpt(t, str(teacher_path))\n", "    rows=[]\n", "    for p,nm in zip(student_paths, names):\n", "        s = make_vgg('vgg11', pretrained=False).to(DEVICE)\n", "        load_ckpt(s, str(p))\n", "        kl = mean_kl_teacher_student(t, s, val_loader, T=T, max_batches=50)\n", "        rows.append({'method':nm, 'mean_kl':kl})\n", "    df = pd.DataFrame(rows)\n", "    path = RES/'kl_alignment.csv'\n", "    df.to_csv(path, index=False)\n", "    print('Saved', path)\n", "    return df\n", "\n", "def color_invariance_experiment(epochs_teacher_ft=10, epochs_student=30):\n", "    # 1) finetune teacher16 with color jitter\n", "    t = make_vgg('vgg16', pretrained=False).to(DEVICE)\n", "    load_ckpt(t, str(CKPTS/'teacher_vgg16.pt'))\n", "    jitter_loader, _ = get_cifar100(color_jitter=True)\n", "    print('Finetuning teacher with color jitter...')\n", "    train_ce(t, jitter_loader, val_loader, epochs=epochs_teacher_ft, lr=0.01)\n", "    save_ckpt(t, str(CKPTS/'teacher_vgg16_colorjitter.pt'))\n", "    # 2) CRD distill to student\n", "    s = make_vgg('vgg11', pretrained=False).to(DEVICE)\n", "    print('Distilling CRD from jittered teacher...')\n", "    train_kd_crd(s, t, train_loader, val_loader, epochs=epochs_student, lr=0.1)\n", "    save_ckpt(s, str(CKPTS/'student_CRD_color.pt'))\n", "    # 3) Evaluate on clean vs jittered val\n", "    _, val_loader_j = get_cifar100(color_jitter=True)\n", "    clean = evaluate(s, val_loader)\n", "    jitter= evaluate(s, val_loader_j)\n", "    df = pd.DataFrame([{'method':'CRD_color', 'clean_top1':clean[1], 'jitter_top1':jitter[1], 'delta': jitter[1]-clean[1]}])\n", "    path = RES/'color_invariance.csv'\n", "    df.to_csv(path, index=False)\n", "    print('Saved', path)\n", "    return df\n", "\n", "def larger_teacher_experiment(epochs=60):\n", "    # Distill LM from VGG-16 vs VGG-19\n", "    # Assumes teacher_vgg16.pt and teacher_vgg19.pt exist\n", "    s16 = distill_lm(teacher_path=CKPTS/'teacher_vgg16.pt', epochs=epochs)\n", "    s19 = distill_lm(teacher_path=CKPTS/'teacher_vgg19.pt', epochs=epochs)\n", "    l1 = evaluate(s16, val_loader)\n", "    l2 = evaluate(s19, val_loader)\n", "    df = pd.DataFrame([\n", "        {'teacher':'vgg16', 'student':'vgg11', 'top1': l1[1], 'top5': l1[2]},\n", "        {'teacher':'vgg19', 'student':'vgg11', 'top1': l2[1], 'top5': l2[2]},\n", "    ])\n", "    path = RES/'larger_teacher.csv'\n", "    df.to_csv(path, index=False)\n", "    print('Saved', path)\n", "    return df\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Grad-CAM utilities (optional; uses `pytorch-grad-cam` if present)"]}, {"cell_type": "code", "metadata": {"id": "gradcam_utils"}, "execution_count": null, "outputs": [], "source": ["def gradcam_grid(models_dict, loader, n_images=3, save_path=FIGS/'gradcam_grid.png'):\n", "    try:\n", "        from pytorch_grad_cam import GradCAM\n", "        from pytorch_grad_cam.utils.image import show_cam_on_image\n", "        import matplotlib.pyplot as plt\n", "    except Exception as e:\n", "        print(\"pytorch-grad-cam not available; skipping Grad-CAM.\")\n", "        return\n", "    # pick targets on the last conv layer for VGG\n", "    def last_conv(model):\n", "        layers = [m for m in model.features if isinstance(m, nn.Conv2d)]\n", "        return layers[-1]\n", "    model_targets = {name: last_conv(m) for name,m in models_dict.items()}\n", "    # sample a few images\n", "    xs=[]; ys=[]\n", "    for i,(x,y) in enumerate(loader):\n", "        xs.append(x); ys.append(y)\n", "        if len(xs)*x.size(0)>=n_images: break\n", "    x = torch.cat(xs, dim=0)[:n_images].to(DEVICE)\n", "    # naive unnormalize to [0,1] for visualization\n", "    img_np = x.detach().cpu().float().clamp(0,1).permute(0,2,3,1).numpy()\n", "    import matplotlib.pyplot as plt\n", "    cols = len(models_dict)\n", "    fig, ax = plt.subplots(n_images, cols, figsize=(3*cols, 3*n_images))\n", "    if n_images==1: ax = np.expand_dims(ax, 0)\n", "    for c,(name,model) in enumerate(models_dict.items()):\n", "        target_layer = model_targets[name]\n", "        cam = GradCAM(model=model, target_layers=[target_layer], use_cuda=torch.cuda.is_available())\n", "        grayscale_cams = cam(input_tensor=x)\n", "        for r in range(n_images):\n", "            vis = show_cam_on_image(img_np[r], grayscale_cams[r], use_rgb=True)\n", "            ax[r,c].imshow(vis); ax[r,c].axis('off')\n", "            if r==0: ax[r,c].set_title(name)\n", "    plt.tight_layout(); plt.savefig(save_path, dpi=160); plt.close()\n", "    print('Saved', save_path)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Plotting (accuracy bars, KL bars)"]}, {"cell_type": "code", "metadata": {"id": "plotting"}, "execution_count": null, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import pandas as pd\n", "\n", "def plot_acc_bar(csv_path=RES/'metrics_overall.csv', out=FIGS/'acc_bar.png'):\n", "    if not Path(csv_path).exists():\n", "        print('metrics_overall.csv not found; skipping.')\n", "        return\n", "    df = pd.read_csv(csv_path)\n", "    methods = df['method']\n", "    acc = df['top1_acc']\n", "    plt.figure(figsize=(6,4))\n", "    plt.bar(methods, acc)\n", "    plt.xticks(rotation=20)\n", "    plt.ylabel('Top-1 Accuracy')\n", "    plt.title('KD Methods \u2014 CIFAR-100')\n", "    plt.tight_layout(); plt.savefig(out, dpi=160); plt.close()\n", "    print('Saved', out)\n", "\n", "def plot_kl_bar(csv_path=RES/'kl_alignment.csv', out=FIGS/'kl_bar.png'):\n", "    if not Path(csv_path).exists():\n", "        print('kl_alignment.csv not found; skipping.')\n", "        return\n", "    df = pd.read_csv(csv_path)\n", "    methods = df['method']\n", "    vals = df['mean_kl']\n", "    plt.figure(figsize=(6,4))\n", "    plt.bar(methods, vals)\n", "    plt.xticks(rotation=20)\n", "    plt.ylabel('Mean KL(T||S)')\n", "    plt.title('Distribution Alignment (lower is better)')\n", "    plt.tight_layout(); plt.savefig(out, dpi=160); plt.close()\n", "    print('Saved', out)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Orchestration (run cells in order in Colab)\n", "\n", "Use these *after* training/finetuning steps to produce CSVs/PNGs:\n", "- `kl_alignment_table()`\n", "- `plot_acc_bar()`, `plot_kl_bar()`\n", "- `gradcam_grid(...)`\n", "- `color_invariance_experiment()`\n", "- `larger_teacher_experiment()`"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}