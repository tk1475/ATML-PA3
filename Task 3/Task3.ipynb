{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "12QM0PJyxNZ-",
        "outputId": "0323141c-a987-41c1-b617-e1dc158bf79e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "12QM0PJyxNZ-",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Imports and seeds"
      ],
      "metadata": {
        "id": "vRuRxlNuhD4I"
      },
      "id": "vRuRxlNuhD4I"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dabe0fc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dabe0fc3",
        "outputId": "599b5fee-f84f-4d0e-8561-c4bbdbe32011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os, random, json, math, time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "USE_AMP = True\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "ROOT = Path('/content/ATML_A3')\n",
        "CKPTS = ROOT / 'ckpts'\n",
        "RES   = ROOT / 'results'\n",
        "FIGS  = ROOT / 'figures'\n",
        "for p in [CKPTS, RES, FIGS]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def set_seed(seed=1337):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(1337)\n",
        "print(f\"Device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Data: CIFAR-100"
      ],
      "metadata": {
        "id": "M_b2qlJxhYof"
      },
      "id": "M_b2qlJxhYof"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cifar100(batch_size=128, num_workers=2):\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std  = (0.2675, 0.2565, 0.2761)\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    test_tf  = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "    train_ds = datasets.CIFAR100(root=str(ROOT/'data'), train=True,  download=True, transform=train_tf)\n",
        "    val_ds   = datasets.CIFAR100(root=str(ROOT/'data'), train=False, download=True, transform=test_tf)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_loader, val_loader = get_cifar100()\n"
      ],
      "metadata": {
        "id": "XRITyu4uhVj5",
        "outputId": "012a1ba0-712e-469c-ecae-823d5fd7f875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XRITyu4uhVj5",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:02<00:00, 73.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Models: VGG-16/19 (teacher), VGG-11 (student)"
      ],
      "metadata": {
        "id": "R3LwB2WJhimR"
      },
      "id": "R3LwB2WJhimR"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_vgg(name='vgg16', num_classes=100, pretrained=False):\n",
        "    if name == 'vgg16':\n",
        "        net = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    elif name == 'vgg19':\n",
        "        net = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    elif name == 'vgg11':\n",
        "        net = models.vgg11(weights=None)\n",
        "    else:\n",
        "        raise ValueError('Unsupported VGG: ' + name)\n",
        "    in_feats = net.classifier[-1].in_features\n",
        "    net.classifier[-1] = nn.Linear(in_feats, num_classes)\n",
        "    return net.to(DEVICE)\n",
        "\n",
        "teacher16 = make_vgg('vgg16', pretrained=True)\n",
        "student11 = make_vgg('vgg11', pretrained=False)\n",
        "print(\"Built VGG-16 (teacher) & VGG-11 (student).\")\n"
      ],
      "metadata": {
        "id": "HOZRQfYuheQp",
        "outputId": "5c0ef931-5a5f-49ca-a72f-0a1c830db3c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HOZRQfYuheQp",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 528M/528M [00:06<00:00, 79.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built VGG-16 (teacher) & VGG-11 (student).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Losses: CE, Label Smoothing, KD-LM, DKD"
      ],
      "metadata": {
        "id": "cJdN5OvphuUz"
      },
      "id": "cJdN5OvphuUz"
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps=0.1):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.logsoft = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, logits, targets):\n",
        "        n = logits.size(1)\n",
        "        logp = self.logsoft(logits)\n",
        "        with torch.no_grad():\n",
        "            dist = torch.zeros_like(logp)\n",
        "            dist.fill_(self.eps / (n - 1))\n",
        "            dist.scatter_(1, targets.unsqueeze(1), 1 - self.eps)\n",
        "        return torch.mean(torch.sum(-dist * logp, dim=1))\n",
        "\n",
        "def kd_loss_logits(student_logits, teacher_logits, T=4.0):\n",
        "    # KL(T||S) with temperature scaling (classic Logit Matching)\n",
        "    p = F.log_softmax(student_logits / T, dim=1)\n",
        "    q = F.softmax(teacher_logits / T, dim=1)\n",
        "    return F.kl_div(p, q, reduction='batchmean') * (T * T)\n",
        "\n",
        "class DKDLoss(nn.Module):\n",
        "    # Minimal DKD: decouples target vs non-target parts\n",
        "    def __init__(self, alpha=1.0, beta=8.0, T=4.0):\n",
        "        super().__init__()\n",
        "        self.alpha, self.beta, self.T = alpha, beta, T\n",
        "    def forward(self, s_logits, t_logits, targets):\n",
        "        T = self.T\n",
        "        s = F.log_softmax(s_logits / T, dim=1)\n",
        "        t = F.softmax(t_logits / T, dim=1)\n",
        "        one_hot = F.one_hot(targets, num_classes=s_logits.size(1)).float()\n",
        "        pos_loss = F.kl_div((s * one_hot).sum(1, keepdim=True),\n",
        "                            (t * one_hot).sum(1, keepdim=True), reduction='batchmean')\n",
        "        neg_loss = F.kl_div((s * (1 - one_hot)), (t * (1 - one_hot)), reduction='batchmean')\n",
        "        return (self.alpha * pos_loss + self.beta * neg_loss) * (T * T)\n"
      ],
      "metadata": {
        "id": "8YJJMf7xhuc3"
      },
      "id": "8YJJMf7xhuc3",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Eval + generic CE training loop"
      ],
      "metadata": {
        "id": "82w_thi5h2Hd"
      },
      "id": "82w_thi5h2Hd"
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_topk(logits, targets, topk=(1,)):\n",
        "    maxk = max(topk); batch_size = targets.size(0)\n",
        "    _, pred = logits.topk(maxk, 1, True, True); pred = pred.t()\n",
        "    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
        "    res=[]\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    loss_sum=0.0; n=0; top1=0.0; top5=0.0\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x=x.to(DEVICE); y=y.to(DEVICE)\n",
        "            logits = model(x)\n",
        "            loss = ce(logits, y)\n",
        "            a1,a5 = accuracy_topk(logits, y, topk=(1,5))\n",
        "            bs = x.size(0)\n",
        "            loss_sum += loss.item()*bs; n += bs\n",
        "            top1 += a1.item()*bs/100.0; top5 += a5.item()*bs/100.0\n",
        "    return loss_sum/n, 100*top1/n, 100*top5/n\n",
        "\n",
        "def train_ce(model, train_loader, val_loader, epochs=60, lr=0.1, weight_decay=5e-4, use_ls=False, ls_eps=0.1):\n",
        "    model.to(DEVICE)\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "    ce = LabelSmoothingCE(ls_eps) if use_ls else nn.CrossEntropyLoss()\n",
        "    best=(1e9,0,0)\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        for x,y in train_loader:\n",
        "            x=x.to(DEVICE); y=y.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "                logits = model(x)\n",
        "                loss = ce(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt); scaler.update()\n",
        "        sched.step()\n",
        "        vl, a1, a5 = evaluate(model, val_loader)\n",
        "        if a1>best[1]: best=(vl,a1,a5)\n",
        "        if ep%10==0 or ep==1:\n",
        "            print(f\"[{'LS-CE' if use_ls else 'CE'}] {ep}/{epochs} | val_loss={vl:.3f} top1={a1:.2f} top5={a5:.2f}\")\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "R6rAcLNbhyok"
      },
      "id": "R6rAcLNbhyok",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) KD-LM training loop"
      ],
      "metadata": {
        "id": "107Y6vFTh555"
      },
      "id": "107Y6vFTh555"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kd_lm(student, teacher, train_loader, val_loader, epochs=60, lr=0.1, alpha=0.5, T=4.0):\n",
        "    student.to(DEVICE); teacher.eval().to(DEVICE)\n",
        "    opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    best=(1e9,0,0)\n",
        "    for ep in range(1, epochs+1):\n",
        "        student.train()\n",
        "        for x,y in train_loader:\n",
        "            x=x.to(DEVICE); y=y.to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                t_logits = teacher(x)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "                s_logits = student(x)\n",
        "                loss = alpha*kd_loss_logits(s_logits, t_logits, T=T) + (1-alpha)*ce(s_logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt); scaler.update()\n",
        "        sched.step()\n",
        "        vl, a1, a5 = evaluate(student, val_loader)\n",
        "        if a1>best[1]: best=(vl,a1,a5)\n",
        "        if ep%10==0 or ep==1:\n",
        "            print(f\"[KD-LM] {ep}/{epochs} | val_loss={vl:.3f} top1={a1:.2f} top5={a5:.2f}\")\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "j-cnF35Lh5Vt"
      },
      "id": "j-cnF35Lh5Vt",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) DKD training loop"
      ],
      "metadata": {
        "id": "dTePTUiBh-Tn"
      },
      "id": "dTePTUiBh-Tn"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dkd(student, teacher, train_loader, val_loader, epochs=60, lr=0.1, alpha=1.0, beta=8.0, T=4.0):\n",
        "    student.to(DEVICE); teacher.eval().to(DEVICE)\n",
        "    dkd = DKDLoss(alpha=alpha, beta=beta, T=T)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "    best=(1e9,0,0)\n",
        "    for ep in range(1, epochs+1):\n",
        "        student.train()\n",
        "        for x,y in train_loader:\n",
        "            x=x.to(DEVICE); y=y.to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                t_logits = teacher(x)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "                s_logits = student(x)\n",
        "                loss = ce(s_logits, y) + dkd(s_logits, t_logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt); scaler.update()\n",
        "        sched.step()\n",
        "        vl, a1, a5 = evaluate(student, val_loader)\n",
        "        if a1>best[1]: best=(vl,a1,a5)\n",
        "        if ep%10==0 or ep==1:\n",
        "            print(f\"[DKD] {ep}/{epochs} | val_loss={vl:.3f} top1={a1:.2f} top5={a5:.2f}\")\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "W2xrnpKAh-pS"
      },
      "id": "W2xrnpKAh-pS",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) Save/Load helpers"
      ],
      "metadata": {
        "id": "cy7PZBhsl-Rg"
      },
      "id": "cy7PZBhsl-Rg"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_ckpt(model, path): torch.save({'state_dict': model.state_dict()}, path)\n",
        "def load_ckpt(model, path): model.load_state_dict(torch.load(path, map_location=DEVICE)['state_dict'])\n"
      ],
      "metadata": {
        "id": "EzmXYuVsl-aL"
      },
      "id": "EzmXYuVsl-aL",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) Orchestrate Task 3.1 (Teacher → SI → LS → LM → DKD)"
      ],
      "metadata": {
        "id": "qQdqOBwLmHcn"
      },
      "id": "qQdqOBwLmHcn"
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 60  # For quick smoke-test use 30, then increase.\n",
        "\n",
        "# 1) Train Teacher (VGG-16)\n",
        "teacher16 = make_vgg('vgg16', pretrained=True)\n",
        "best_T = train_ce(teacher16, train_loader, val_loader, epochs=EPOCHS, lr=0.1)\n",
        "save_ckpt(teacher16, str(CKPTS/'teacher_vgg16.pt'))\n",
        "print(\"Teacher saved:\", best_T)\n",
        "\n",
        "# 2) Independent Student (VGG-11) baseline\n",
        "si = make_vgg('vgg11', pretrained=False)\n",
        "best_SI = train_ce(si, train_loader, val_loader, epochs=EPOCHS, lr=0.1)\n",
        "save_ckpt(si, str(CKPTS/'student_SI.pt'))\n",
        "print(\"SI saved:\", best_SI)\n",
        "\n",
        "# 3) Label Smoothing baseline (VGG-11)\n",
        "ls = make_vgg('vgg11', pretrained=False)\n",
        "best_LS = train_ce(ls, train_loader, val_loader, epochs=EPOCHS, lr=0.1, use_ls=True, ls_eps=0.1)\n",
        "save_ckpt(ls, str(CKPTS/'student_LS.pt'))\n",
        "print(\"LS saved:\", best_LS)\n",
        "\n",
        "# 4) KD — Basic Logit Matching (VGG-11 distilled from VGG-16)\n",
        "lm = make_vgg('vgg11', pretrained=False)\n",
        "best_LM = train_kd_lm(lm, teacher16, train_loader, val_loader, epochs=EPOCHS, lr=0.1, alpha=0.5, T=4.0)\n",
        "save_ckpt(lm, str(CKPTS/'student_LM.pt'))\n",
        "print(\"LM saved:\", best_LM)\n",
        "\n",
        "# 5) KD — Decoupled KD (VGG-11 distilled from VGG-16)\n",
        "dkd = make_vgg('vgg11', pretrained=False)\n",
        "best_DKD = train_dkd(dkd, teacher16, train_loader, val_loader, epochs=EPOCHS, lr=0.1, alpha=1.0, beta=8.0, T=4.0)\n",
        "save_ckpt(dkd, str(CKPTS/'student_DKD.pt'))\n",
        "print(\"DKD saved:\", best_DKD)\n"
      ],
      "metadata": {
        "id": "Ob0ZNXiLmDkM",
        "outputId": "cff67271-9f6f-4447-d64b-ff1653da902a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ob0ZNXiLmDkM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1192823495.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
            "/tmp/ipython-input-1192823495.py:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=USE_AMP):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CE] 1/60 | val_loss=4.185 top1=3.94 top5=16.51\n",
            "[CE] 10/60 | val_loss=3.615 top1=10.73 top5=36.56\n",
            "[CE] 20/60 | val_loss=3.065 top1=23.00 top5=54.52\n",
            "[CE] 30/60 | val_loss=2.441 top1=36.78 top5=69.43\n",
            "[CE] 40/60 | val_loss=1.816 top1=52.38 top5=80.72\n",
            "[CE] 50/60 | val_loss=1.409 top1=63.54 top5=86.53\n",
            "[CE] 60/60 | val_loss=1.370 top1=67.15 top5=88.74\n",
            "Teacher saved: (1.3701172494888305, 67.15, 88.74)\n",
            "[CE] 1/60 | val_loss=4.607 top1=1.00 top5=5.00\n",
            "[CE] 10/60 | val_loss=3.714 top1=10.56 top5=35.29\n",
            "[CE] 20/60 | val_loss=3.116 top1=23.86 top5=52.75\n",
            "[CE] 30/60 | val_loss=2.496 top1=37.69 top5=67.73\n",
            "[CE] 40/60 | val_loss=1.896 top1=51.73 top5=78.81\n",
            "[CE] 50/60 | val_loss=1.503 top1=61.32 top5=85.32\n",
            "[CE] 60/60 | val_loss=1.510 top1=64.05 top5=87.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10) Quick metrics dump (for your report table/plot later)"
      ],
      "metadata": {
        "id": "tsBRJdkdmTPz"
      },
      "id": "tsBRJdkdmTPz"
    },
    {
      "cell_type": "code",
      "source": [
        "def snapshot_metrics(method_name, model):\n",
        "    vl,a1,a5 = evaluate(model, val_loader)\n",
        "    return {'method':method_name, 'top1_acc':a1, 'top5_acc':a5, 'val_loss':vl}\n",
        "\n",
        "rows = []\n",
        "# Reload to be safe (in case you restart cells later)\n",
        "T  = make_vgg('vgg16'); load_ckpt(T,  str(CKPTS/'teacher_vgg16.pt'))\n",
        "SI = make_vgg('vgg11'); load_ckpt(SI, str(CKPTS/'student_SI.pt'))\n",
        "LS = make_vgg('vgg11'); load_ckpt(LS, str(CKPTS/'student_LS.pt'))\n",
        "LM = make_vgg('vgg11'); load_ckpt(LM, str(CKPTS/'student_LM.pt'))\n",
        "DK = make_vgg('vgg11'); load_ckpt(DK, str(CKPTS/'student_DKD.pt'))\n",
        "\n",
        "for name, m in [('SI',SI), ('LS',LS), ('LM',LM), ('DKD',DK)]:\n",
        "    rows.append(snapshot_metrics(name, m))\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(RES/'task3_part1_metrics.csv', index=False)\n",
        "print(df)\n",
        "print(\"Saved:\", RES/'task3_part1_metrics.csv')\n"
      ],
      "metadata": {
        "id": "ztvP4MdPmMn3"
      },
      "id": "ztvP4MdPmMn3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}