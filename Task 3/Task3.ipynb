{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"vRuRxlNuhD4I","cell_type":"markdown","source":"## 1) Imports and seeds","metadata":{"id":"vRuRxlNuhD4I"}},{"id":"dabe0fc3","cell_type":"code","source":"# Cell 1 — Imports / Config / Seeds\nimport os, math, json, copy, random\nimport numpy as np\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\n\ndef set_seed(s=42):\n    random.seed(s); np.random.seed(s)\n    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.benchmark = True\nset_seed(42)\n\nCFG = dict(\n    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n    save_dir = './ckpts_task3',\n    epochs = 50, batch_size = 256, num_workers = 4,\n    lr = 0.1, weight_decay = 5e-4, warmup_epochs = 5,\n    ema_decay = 0.999, label_smoothing = 0.1,\n    kd_alpha_ce = 0.6, kd_T = 4.0, dkd_tau = 0.5,\n    use_randaugment = True, download_if_missing = True\n)\nos.makedirs(CFG[\"save_dir\"], exist_ok=True)\n\nCIFAR100_MEAN=(0.5071,0.4867,0.4408); CIFAR100_STD=(0.2675,0.2565,0.2761)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dabe0fc3","outputId":"a13b2cb9-3ad3-47a2-df7f-9bf19396333c","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:26:35.402016Z","iopub.execute_input":"2025-11-02T04:26:35.402688Z","iopub.status.idle":"2025-11-02T04:26:35.410016Z","shell.execute_reply.started":"2025-11-02T04:26:35.402666Z","shell.execute_reply":"2025-11-02T04:26:35.409253Z"}},"outputs":[],"execution_count":6},{"id":"M_b2qlJxhYof","cell_type":"markdown","source":"## 2) Data: CIFAR-100","metadata":{"id":"M_b2qlJxhYof"}},{"id":"XRITyu4uhVj5","cell_type":"code","source":"# Cell 2 — Data\nfrom pathlib import Path\ndef tfs(train=True, randaug=True, color=False):\n    aug = [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()]\n    if color: aug.append(transforms.ColorJitter(0.4,0.4,0.4,0.1))\n    if randaug and train:\n        try:\n            from torchvision.transforms.autoaugment import RandAugment\n            aug.append(RandAugment(num_ops=2, magnitude=10))\n        except: pass\n    base = [transforms.ToTensor(), transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD)]\n    return transforms.Compose((aug if train else []) + base)\n\ndef get_loaders(train_tf, val_tf, root='./data', download=True):\n    tr = datasets.CIFAR100(root, train=True, transform=train_tf, download=download)\n    va = datasets.CIFAR100(root, train=False, transform=val_tf, download=download)\n    return (DataLoader(tr, batch_size=CFG[\"batch_size\"], shuffle=True, num_workers=CFG[\"num_workers\"], pin_memory=True),\n            DataLoader(va, batch_size=CFG[\"batch_size\"], shuffle=False, num_workers=CFG[\"num_workers\"], pin_memory=True))\n\ntrain_loader, val_loader = get_loaders(tfs(train=True, randaug=CFG[\"use_randaugment\"]), tfs(train=False))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRITyu4uhVj5","outputId":"67ca0c83-e835-4452-ad60-779c7e059d3b","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:26:35.411405Z","iopub.execute_input":"2025-11-02T04:26:35.411663Z","iopub.status.idle":"2025-11-02T04:26:37.219721Z","shell.execute_reply.started":"2025-11-02T04:26:35.411644Z","shell.execute_reply":"2025-11-02T04:26:37.219137Z"}},"outputs":[],"execution_count":7},{"id":"R3LwB2WJhimR","cell_type":"markdown","source":"## 3) Models: VGG-16/19 (teacher), VGG-11 (student)","metadata":{"id":"R3LwB2WJhimR"}},{"id":"HOZRQfYuheQp","cell_type":"code","source":"# Cell 3 — Models\ndef build_vgg11_student(nc=100):\n    m = models.vgg11_bn(weights=None)\n    m.classifier[-1] = nn.Linear(m.classifier[-1].in_features, nc)\n    return m\n\ndef build_vgg_teacher(depth='vgg16_bn', nc=100, pretrained=True):\n    assert depth in ['vgg16_bn','vgg19_bn']\n    w = (models.VGG16_BN_Weights.IMAGENET1K_V1 if depth=='vgg16_bn' else models.VGG19_BN_Weights.IMAGENET1K_V1) if pretrained else None\n    m = getattr(models, depth)(weights=w)\n    m.classifier[-1] = nn.Linear(m.classifier[-1].in_features, nc)\n    return m\n\n@torch.no_grad()\ndef forward_logits_penult(m, x):\n    # returns logits and penultimate (before last Linear)\n    feats = m.features(x); feats = torch.flatten(feats, 1)\n    pen = None\n    h = feats\n    for i, layer in enumerate(m.classifier):\n        h = layer(h)\n        if i == len(m.classifier)-2: pen = h.clone()\n    return h, pen\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOZRQfYuheQp","outputId":"582aac00-fd53-4482-f619-30159fc501a7","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:26:37.221184Z","iopub.execute_input":"2025-11-02T04:26:37.221436Z","iopub.status.idle":"2025-11-02T04:26:37.227602Z","shell.execute_reply.started":"2025-11-02T04:26:37.221418Z","shell.execute_reply":"2025-11-02T04:26:37.226935Z"}},"outputs":[],"execution_count":8},{"id":"cJdN5OvphuUz","cell_type":"markdown","source":"## 4) Losses: LS, LM(KL), DKD, Hints, CRD","metadata":{"id":"cJdN5OvphuUz"}},{"id":"8YJJMf7xhuc3","cell_type":"code","source":"# Cell 4 — Losses\nclass LSCELoss(nn.Module):\n    def __init__(self, eps=0.1): super().__init__(); self.eps=eps\n    def forward(self, logits, y):\n        n = logits.size(-1); logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            t = torch.zeros_like(logp).fill_(self.eps/(n-1))\n            t.scatter_(1, y.unsqueeze(1), 1-self.eps)\n        return torch.mean(torch.sum(-t*logp, dim=-1))\n\nclass DistillKLLoss(nn.Module):\n    def __init__(self, T=4.0): super().__init__(); self.T=T; self.kl=nn.KLDivLoss(reduction='batchmean')\n    def forward(self, s, t):\n        return self.kl(F.log_softmax(s/self.T,dim=1), F.softmax(t/self.T,dim=1))*(self.T**2)\n\nclass DKDLoss(nn.Module):\n    def __init__(self, alpha_ce=0.6, tau=0.5, T=4.0, eps_ls=0.1):\n        super().__init__(); self.alpha_ce=alpha_ce; self.tau=tau; self.T=T; self.lsce=LSCELoss(eps_ls); self.kl=nn.KLDivLoss(reduction='batchmean')\n    def forward(self, s, t, y):\n        loss_ce = self.lsce(s, y)\n        ps = F.log_softmax(s/self.T, dim=1); pt = F.softmax(t/self.T, dim=1)\n        mask_t = torch.zeros_like(pt).scatter_(1, y.view(-1,1), 1.0); mask_nt = 1-mask_t\n        ps_t = (ps*mask_t).sum(1, keepdim=True); pt_t = (pt*mask_t).sum(1, keepdim=True)\n        ps_nt = F.log_softmax(ps + (mask_t*(-1e9)), dim=1)\n        pt_nt = (pt*mask_nt); pt_nt = pt_nt / pt_nt.sum(1, keepdim=True).clamp_min(1e-12)\n        loss_t = F.kl_div(ps_t, pt_t, reduction='batchmean', log_target=False)\n        loss_nt = self.kl(ps_nt, pt_nt)\n        return self.alpha_ce*loss_ce + (1-self.alpha_ce)*(self.T**2)*(self.tau*loss_t + (1-self.tau)*loss_nt)\n\nclass HintRegressor(nn.Module):\n    def __init__(self, in_ch, out_ch): super().__init__(); self.proj=nn.Conv2d(in_ch,out_ch,1,bias=False)\n    def forward(self, x): return self.proj(x)\n\nclass HintsLoss(nn.Module):\n    def __init__(self, adapter, w=1.0): super().__init__(); self.adapter=adapter; self.w=w; self.mse=nn.MSELoss()\n    def forward(self, feat_s, feat_t): return self.w*self.mse(self.adapter(feat_s), feat_t.detach())\n\nclass CRDHead(nn.Module):\n    def __init__(self, in_dim, out_dim=128):\n        super().__init__(); self.proj=nn.Sequential(nn.Linear(in_dim,512), nn.ReLU(True), nn.Linear(512,out_dim))\n    def forward(self, x): return F.normalize(self.proj(x), dim=1)\n\ndef crd_loss(zs, zt, temp=0.07):\n    logits = zs @ zt.t() / temp\n    labels = torch.arange(zs.size(0), device=zs.device)\n    return F.cross_entropy(logits, labels)\n","metadata":{"id":"8YJJMf7xhuc3","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:26:37.228334Z","iopub.execute_input":"2025-11-02T04:26:37.228603Z","iopub.status.idle":"2025-11-02T04:26:37.244981Z","shell.execute_reply.started":"2025-11-02T04:26:37.228587Z","shell.execute_reply":"2025-11-02T04:26:37.244306Z"}},"outputs":[],"execution_count":9},{"id":"82w_thi5h2Hd","cell_type":"markdown","source":"## 5) Training utilities: accuracy, EMA, warmup+cosine, evaluate","metadata":{"id":"82w_thi5h2Hd"}},{"id":"R6rAcLNbhyok","cell_type":"code","source":"# Cell 5 — Utils (acc/EMA/scheduler/eval)\ndef accuracy(logits, y, topk=(1,)):\n    maxk=max(topk); b=y.size(0)\n    _, pred = logits.topk(maxk,1,True,True); pred=pred.t()\n    correct = pred.eq(y.view(1,-1).expand_as(pred))\n    res=[]\n    for k in topk:\n        res.append(correct[:k].reshape(-1).float().sum().mul_(100.0/b).item())\n    return res\n\nclass EMA:\n    def __init__(self, m, decay=0.999):\n        self.decay=decay; self.shadow={n:p.detach().clone() for n,p in m.named_parameters() if p.requires_grad}\n    @torch.no_grad()\n    def update(self, m):\n        for n,p in m.named_parameters():\n            if p.requires_grad: self.shadow[n]=self.decay*self.shadow[n]+(1-self.decay)*p.detach()\n    @torch.no_grad()\n    def copy_to(self, m):\n        for n,p in m.named_parameters():\n            if p.requires_grad: p.data.copy_(self.shadow[n])\n\nclass WarmupCosine:\n    def __init__(self, opt, base_lr, warm, maxe): self.opt=opt; self.base=base_lr; self.warm=max(1,warm); self.maxe=maxe; self.e=0\n    def step(self):\n        self.e+=1\n        for pg in self.opt.param_groups:\n            if self.e<=self.warm: lr=self.base*self.e/self.warm\n            else:\n                t=(self.e-self.warm)/max(1,(self.maxe-self.warm)); lr=0.5*self.base*(1+math.cos(math.pi*t))\n            pg['lr']=lr\n\n@torch.no_grad()\ndef evaluate(model, loader, device):\n    model.eval(); ce=nn.CrossEntropyLoss()\n    losses=[]; t1=[]; t5=[]\n    for x,y in loader:\n        x,y=x.to(device),y.to(device)\n        out=model(x); losses.append(ce(out,y).item())\n        a1,a5=accuracy(out,y,(1,5)); t1.append(a1); t5.append(a5)\n    return dict(val_loss=float(np.mean(losses)), top1=float(np.mean(t1)), top5=float(np.mean(t5)))\n","metadata":{"id":"R6rAcLNbhyok","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:26:37.245788Z","iopub.execute_input":"2025-11-02T04:26:37.246040Z","iopub.status.idle":"2025-11-02T04:26:37.260829Z","shell.execute_reply.started":"2025-11-02T04:26:37.246016Z","shell.execute_reply":"2025-11-02T04:26:37.260256Z"}},"outputs":[],"execution_count":10},{"id":"107Y6vFTh555","cell_type":"markdown","source":"## 6) Core train loop that supports: SI (LS), LM, DKD, Hints, CRD","metadata":{"id":"107Y6vFTh555"}},{"id":"j-cnF35Lh5Vt","cell_type":"code","source":"# Cell 6 — Train loop\nfrom torch.cuda.amp import GradScaler, autocast\n\ndef train_model(mode, student, train_loader, val_loader, device,\n                teacher=None, hints_cfg=None, crd_cfg=None, run_name=\"run\"):\n    student.to(device)\n    if teacher is not None:\n        teacher.to(device); teacher.eval()\n        for p in teacher.parameters(): p.requires_grad=False\n\n    opt = torch.optim.SGD(student.parameters(), lr=CFG[\"lr\"], momentum=0.9, weight_decay=CFG[\"weight_decay\"], nesterov=True)\n    sch = WarmupCosine(opt, CFG[\"lr\"], CFG[\"warmup_epochs\"], CFG[\"epochs\"])\n    scaler=GradScaler(); ema=EMA(student, CFG[\"ema_decay\"])\n\n    ls = LSCELoss(CFG[\"label_smoothing\"])\n    kd = DistillKLLoss(CFG[\"kd_T\"])\n    dkd = DKDLoss(CFG[\"kd_alpha_ce\"], CFG[\"dkd_tau\"], CFG[\"kd_T\"], CFG[\"label_smoothing\"])\n\n    # hints\n    hint_loss=None; feat_s=None; feat_t=None; handles=[]\n    if mode==\"hints\":\n        # pick layers that match (verify names on your VGGs)\n        s_name=hints_cfg[\"s_layer\"]; t_name=hints_cfg[\"t_layer\"]\n        s_feats={}; t_feats={}\n        def hook_s(m,i,o): s_feats[\"f\"]=o\n        def hook_t(m,i,o): t_feats[\"f\"]=o\n        handles.append(dict(student.named_modules())[s_name].register_forward_hook(hook_s))\n        handles.append(dict(teacher.named_modules())[t_name].register_forward_hook(hook_t))\n        hint_loss=HintsLoss(hints_cfg[\"adapter\"], hints_cfg.get(\"w\",1.0))\n        hint_loss.adapter.to(device) \n        feat_s=s_feats; feat_t=t_feats\n\n    # crd\n    head_s=None; head_t=None\n    if mode==\"crd\":\n        head_s=CRDHead(crd_cfg[\"penult_dim\"]).to(device)\n        head_t=CRDHead(crd_cfg[\"penult_dim\"]).to(device)\n        for p in head_t.parameters(): p.requires_grad=False\n\n    hist={\"epoch\":[], \"train_loss\":[], \"val_loss\":[], \"top1\":[], \"top5\":[]}\n    best={\"metric\":-1, \"epoch\":-1, \"sd\":None}\n\n    for ep in range(1, CFG[\"epochs\"]+1):\n        student.train(); train_losses=[]\n        for xb,yb in train_loader:\n            xb,yb=xb.to(device), yb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with autocast():\n                if mode==\"si\":\n                    out = student(xb); loss = ls(out,yb)\n                elif mode==\"lm\":\n                    out = student(xb); tout = teacher(xb)\n                    loss = CFG[\"kd_alpha_ce\"]*ls(out,yb) + (1-CFG[\"kd_alpha_ce\"])*kd(out,tout)\n                elif mode==\"dkd\":\n                    out = student(xb); tout = teacher(xb)\n                    loss = dkd(out, tout, yb)\n                elif mode==\"hints\":\n                    out = student(xb); _ = teacher(xb)\n                    loss = ls(out,yb) + hint_loss(feat_s[\"f\"], feat_t[\"f\"])\n                elif mode==\"crd\":\n                    logits_s, pen_s = forward_logits_penult(student, xb)\n                    with torch.no_grad():\n                        logits_t, pen_t = forward_logits_penult(teacher, xb)\n                    loss = (CFG[\"kd_alpha_ce\"]*ls(logits_s,yb) +\n                            (1-CFG[\"kd_alpha_ce\"])*kd(logits_s,logits_t) +\n                            crd_loss(head_s(pen_s), head_t(pen_t)))\n                else:\n                    raise ValueError(\"bad mode\")\n            scaler.scale(loss).backward()\n            scaler.step(opt); scaler.update()\n            ema.update(student)\n            train_losses.append(loss.item())\n        sch.step()\n\n        # eval with EMA\n        eval_m = copy.deepcopy(student).to(device); ema.copy_to(eval_m)\n        mtr = evaluate(eval_m, val_loader, device)\n        hist[\"epoch\"].append(ep); hist[\"train_loss\"].append(float(np.mean(train_losses)))\n        hist[\"val_loss\"].append(mtr[\"val_loss\"]); hist[\"top1\"].append(mtr[\"top1\"]); hist[\"top5\"].append(mtr[\"top5\"])\n        if mtr[\"top1\"]>best[\"metric\"]:\n            best.update(metric=mtr[\"top1\"], epoch=ep, sd=copy.deepcopy(eval_m.state_dict()))\n            torch.save(best[\"sd\"], os.path.join(CFG[\"save_dir\"], f\"{run_name}_best.pt\"))\n        if ep in [1,10,20,30,40,50,CFG[\"epochs\"]]:\n            print(f\"[{run_name}] {ep}/{CFG['epochs']} val_loss={mtr['val_loss']:.3f} top1={mtr['top1']:.2f} top5={mtr['top5']:.2f}\")\n\n    with open(os.path.join(CFG[\"save_dir\"], f\"{run_name}_history.json\"),\"w\") as f: json.dump(hist,f)\n    return hist, best\n","metadata":{"id":"j-cnF35Lh5Vt","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:26:37.262897Z","iopub.execute_input":"2025-11-02T04:26:37.263088Z","iopub.status.idle":"2025-11-02T04:26:37.277933Z","shell.execute_reply.started":"2025-11-02T04:26:37.263074Z","shell.execute_reply":"2025-11-02T04:26:37.277403Z"}},"outputs":[],"execution_count":11},{"id":"dTePTUiBh-Tn","cell_type":"markdown","source":"## 7) Train teachers (VGG-16 std, VGG-19 std, VGG-16-Color)","metadata":{"id":"dTePTUiBh-Tn"}},{"id":"W2xrnpKAh-pS","cell_type":"code","source":"# Cell 7 — Train teachers\nteacher16 = build_vgg_teacher('vgg16_bn', pretrained=True).to(CFG[\"device\"])\nteacher19 = build_vgg_teacher('vgg19_bn', pretrained=True).to(CFG[\"device\"])\n\ntl_tr, tl_va = get_loaders(tfs(train=True, randaug=CFG[\"use_randaugment\"]), tfs(False))\nhist_T16, best_T16 = train_model(\"si\", teacher16, tl_tr, tl_va, CFG[\"device\"], run_name=\"T16_finetune\")\nhist_T19, best_T19 = train_model(\"si\", teacher19, tl_tr, tl_va, CFG[\"device\"], run_name=\"T19_finetune\")\n\n# Color-invariance teacher (VGG-16 with ColorJitter)\ntl_tr_color, tl_va_std = get_loaders(tfs(train=True, randaug=False, color=True), tfs(False))\nteacher16_color = build_vgg_teacher('vgg16_bn', pretrained=True).to(CFG[\"device\"])\nhist_T16C, best_T16C = train_model(\"si\", teacher16_color, tl_tr_color, tl_va_std, CFG[\"device\"], run_name=\"T16_color_finetune\")\n","metadata":{"id":"W2xrnpKAh-pS","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T04:26:37.278593Z","iopub.execute_input":"2025-11-02T04:26:37.278793Z","iopub.status.idle":"2025-11-02T06:38:44.136517Z","shell.execute_reply.started":"2025-11-02T04:26:37.278777Z","shell.execute_reply":"2025-11-02T06:38:44.135698Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n100%|██████████| 528M/528M [00:07<00:00, 72.6MB/s] \nDownloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to /root/.cache/torch/hub/checkpoints/vgg19_bn-c79401a0.pth\n100%|██████████| 548M/548M [00:06<00:00, 88.5MB/s] \n/tmp/ipykernel_37/113450165.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler=GradScaler(); ema=EMA(student, CFG[\"ema_decay\"])\n/tmp/ipykernel_37/113450165.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"[T16_finetune] 1/50 val_loss=4.618 top1=0.98 top5=4.96\n[T16_finetune] 10/50 val_loss=4.052 top1=11.26 top5=27.55\n[T16_finetune] 20/50 val_loss=1.502 top1=61.74 top5=86.02\n[T16_finetune] 30/50 val_loss=1.349 top1=66.32 top5=88.88\n[T16_finetune] 40/50 val_loss=1.382 top1=67.42 top5=88.98\n[T16_finetune] 50/50 val_loss=1.094 top1=73.33 top5=92.78\n[T19_finetune] 1/50 val_loss=4.618 top1=0.98 top5=5.37\n[T19_finetune] 10/50 val_loss=4.645 top1=4.72 top5=13.93\n[T19_finetune] 20/50 val_loss=1.560 top1=59.06 top5=85.36\n[T19_finetune] 30/50 val_loss=1.628 top1=61.45 top5=85.27\n[T19_finetune] 40/50 val_loss=1.471 top1=65.39 top5=87.94\n[T19_finetune] 50/50 val_loss=1.145 top1=72.12 top5=91.82\n[T16_color_finetune] 1/50 val_loss=4.615 top1=1.17 top5=5.15\n[T16_color_finetune] 10/50 val_loss=3.967 top1=14.56 top5=37.74\n[T16_color_finetune] 20/50 val_loss=1.453 top1=63.57 top5=87.48\n[T16_color_finetune] 30/50 val_loss=1.514 top1=64.38 top5=86.68\n[T16_color_finetune] 40/50 val_loss=1.536 top1=65.97 top5=86.17\n[T16_color_finetune] 50/50 val_loss=1.189 top1=72.94 top5=91.07\n","output_type":"stream"}],"execution_count":12},{"id":"cy7PZBhsl-Rg","cell_type":"markdown","source":"## 8) Train students: SI(LS), LM, DKD, Hints, CRD, LM from VGG-19, CRD from color teacher","metadata":{"id":"cy7PZBhsl-Rg"}},{"id":"EzmXYuVsl-aL","cell_type":"code","source":"# Cell 8 — Train students\nSI = build_vgg11_student().to(CFG[\"device\"])\nhist_SI, best_SI = train_model(\"si\", SI, train_loader, val_loader, CFG[\"device\"], run_name=\"S_VGG11_SI\")\n\n# LM from VGG-16\nS_LM = build_vgg11_student().to(CFG[\"device\"]); T16=build_vgg_teacher('vgg16_bn',pretrained=False)\nT16.load_state_dict(torch.load(os.path.join(CFG[\"save_dir\"],\"T16_finetune_best.pt\"), map_location=CFG[\"device\"]))\nhist_LM, best_LM = train_model(\"lm\", S_LM, train_loader, val_loader, CFG[\"device\"], teacher=T16, run_name=\"S_VGG11_LM_T16\")\n\n# DKD from VGG-16\nS_DKD = build_vgg11_student().to(CFG[\"device\"])\nhist_DKD, best_DKD = train_model(\"dkd\", S_DKD, train_loader, val_loader, CFG[\"device\"], teacher=T16, run_name=\"S_VGG11_DKD_T16\")\n\n\n","metadata":{"id":"EzmXYuVsl-aL","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T06:38:44.137752Z","iopub.execute_input":"2025-11-02T06:38:44.138100Z","iopub.status.idle":"2025-11-02T08:23:34.118524Z","shell.execute_reply.started":"2025-11-02T06:38:44.138067Z","shell.execute_reply":"2025-11-02T08:23:34.117508Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/113450165.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler=GradScaler(); ema=EMA(student, CFG[\"ema_decay\"])\n/tmp/ipykernel_37/113450165.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"[S_VGG11_SI] 1/50 val_loss=4.611 top1=0.98 top5=4.88\n[S_VGG11_SI] 10/50 val_loss=60.434 top1=1.07 top5=5.34\n[S_VGG11_SI] 20/50 val_loss=5086.112 top1=1.11 top5=5.25\n[S_VGG11_SI] 30/50 val_loss=61.910 top1=0.98 top5=6.50\n[S_VGG11_SI] 40/50 val_loss=2.785 top1=30.62 top5=62.83\n[S_VGG11_SI] 50/50 val_loss=1.642 top1=56.69 top5=83.39\n[S_VGG11_LM_T16] 1/50 val_loss=4.606 top1=0.98 top5=4.72\n[S_VGG11_LM_T16] 10/50 val_loss=4.612 top1=0.98 top5=5.63\n[S_VGG11_LM_T16] 20/50 val_loss=21.199 top1=2.38 top5=12.51\n[S_VGG11_LM_T16] 30/50 val_loss=1.994 top1=54.70 top5=81.88\n[S_VGG11_LM_T16] 40/50 val_loss=1.569 top1=64.39 top5=87.88\n[S_VGG11_LM_T16] 50/50 val_loss=1.259 top1=69.41 top5=90.70\n[S_VGG11_DKD_T16] 1/50 val_loss=4.607 top1=1.12 top5=5.03\n[S_VGG11_DKD_T16] 10/50 val_loss=6.137 top1=0.99 top5=5.11\n[S_VGG11_DKD_T16] 20/50 val_loss=4.701 top1=0.88 top5=5.05\n[S_VGG11_DKD_T16] 30/50 val_loss=4.605 top1=0.86 top5=4.87\n[S_VGG11_DKD_T16] 40/50 val_loss=4.605 top1=0.98 top5=4.88\n[S_VGG11_DKD_T16] 50/50 val_loss=4.605 top1=0.98 top5=4.88\n","output_type":"stream"}],"execution_count":13},{"id":"51409555-76ce-491e-89eb-b45da1a7961e","cell_type":"code","source":"# Hints (choose layers; verify names by printing model.named_modules())\n# Common choice: teacher features.28 vs student features.20 (adjust if mismatched)\nadapter = HintRegressor(in_ch=512, out_ch=512)\nhcfg = dict(s_layer=\"features.20\", t_layer=\"features.28\", adapter=adapter, w=1.0)\nS_HINT = build_vgg11_student().to(CFG[\"device\"])\nhist_HINT, best_HINT = train_model(\"hints\", S_HINT, train_loader, val_loader, CFG[\"device\"], teacher=T16, hints_cfg=hcfg, run_name=\"S_VGG11_HINTS_T16\")\n\n# CRD from VGG-16 (penultimate dim is 4096 for VGGs with BN)\nS_CRD = build_vgg11_student().to(CFG[\"device\"])\nhist_CRD, best_CRD = train_model(\"crd\", S_CRD, train_loader, val_loader, CFG[\"device\"], teacher=T16,\n                                 crd_cfg=dict(penult_dim=4096), run_name=\"S_VGG11_CRD_T16\")\n\n# LM from VGG-19\nT19=build_vgg_teacher('vgg19_bn',pretrained=False)\nT19.load_state_dict(torch.load(os.path.join(CFG[\"save_dir\"],\"T19_finetune_best.pt\"), map_location=CFG[\"device\"]))\nS_LM19 = build_vgg11_student().to(CFG[\"device\"])\nhist_LM19, best_LM19 = train_model(\"lm\", S_LM19, train_loader, val_loader, CFG[\"device\"], teacher=T19, run_name=\"S_VGG11_LM_T19\")\n\n# CRD from Color teacher\nT16C=build_vgg_teacher('vgg16_bn',pretrained=False)\nT16C.load_state_dict(torch.load(os.path.join(CFG[\"save_dir\"],\"T16_color_finetune_best.pt\"), map_location=CFG[\"device\"]))\nS_CRD_color = build_vgg11_student().to(CFG[\"device\"])\nhist_CRDcol, best_CRDcol = train_model(\"crd\", S_CRD_color, train_loader, val_loader, CFG[\"device\"], teacher=T16C,\n                                       crd_cfg=dict(penult_dim=4096), run_name=\"S_VGG11_CRD_T16Color\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:23:34.119943Z","iopub.execute_input":"2025-11-02T08:23:34.120343Z","iopub.status.idle":"2025-11-02T08:23:36.997120Z","shell.execute_reply.started":"2025-11-02T08:23:34.120316Z","shell.execute_reply":"2025-11-02T08:23:36.995272Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/113450165.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler=GradScaler(); ema=EMA(student, CFG[\"ema_decay\"])\n/tmp/ipykernel_37/113450165.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2251681540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features.20\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features.28\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mS_HINT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vgg11_student\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhist_HINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_HINT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hints\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_HINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhints_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"S_VGG11_HINTS_T16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# CRD from VGG-16 (penultimate dim is 4096 for VGGs with BN)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/113450165.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(mode, student, train_loader, val_loader, device, teacher, hints_cfg, crd_cfg, run_name)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"hints\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhint_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"crd\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mlogits_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_logits_penult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/2032779962.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feat_s, feat_t)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHintsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCRDHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/2032779962.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHintRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_ch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_ch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_ch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHintsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.HalfTensor) and weight type (torch.FloatTensor) should be the same"],"ename":"RuntimeError","evalue":"Input type (torch.cuda.HalfTensor) and weight type (torch.FloatTensor) should be the same","output_type":"error"}],"execution_count":14},{"id":"qQdqOBwLmHcn","cell_type":"markdown","source":"# 9) Summary table (top-1/top-5/loss @ best)","metadata":{"id":"qQdqOBwLmHcn"}},{"id":"Ob0ZNXiLmDkM","cell_type":"code","source":"# Cell 9 — Summary table\nimport pandas as pd, json, glob\ndef load_hist(name):\n    with open(os.path.join(CFG[\"save_dir\"], f\"{name}_history.json\")) as f: return json.load(f)\ndef best_row(tag, hist_path, best_ckpt):\n    h = load_hist(hist_path); e = int(torch.load(os.path.join(CFG[\"save_dir\"], f\"{best_ckpt}_best.pt\"), map_location='cpu').get('epoch', 0) or max(h[\"epoch\"]))\n    idx = h[\"epoch\"].index(e)\n    return dict(run=tag, best_epoch=e, top1=round(h[\"top1\"][idx],2), top5=round(h[\"top5\"][idx],2), val_loss=round(h[\"val_loss\"][idx],4))\n\nrows = [\n    best_row(\"SI\", \"S_VGG11_SI\", \"S_VGG11_SI\"),\n    best_row(\"LM_T16\", \"S_VGG11_LM_T16\", \"S_VGG11_LM_T16\"),\n    best_row(\"DKD_T16\", \"S_VGG11_DKD_T16\", \"S_VGG11_DKD_T16\"),\n    best_row(\"HINTS_T16\", \"S_VGG11_HINTS_T16\", \"S_VGG11_HINTS_T16\"),\n    best_row(\"CRD_T16\", \"S_VGG11_CRD_T16\", \"S_VGG11_CRD_T16\"),\n    best_row(\"LM_T19\", \"S_VGG11_LM_T19\", \"S_VGG11_LM_T19\"),\n    best_row(\"CRD_T16Color\", \"S_VGG11_CRD_T16Color\", \"S_VGG11_CRD_T16Color\"),\n]\ndf = pd.DataFrame(rows).sort_values(\"top1\", ascending=False)\nprint(df.to_string(index=False))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"Ob0ZNXiLmDkM","outputId":"5bd57dd2-d38e-407d-b630-434d733b409a","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:23:36.998111Z","iopub.status.idle":"2025-11-02T08:23:36.998470Z","shell.execute_reply.started":"2025-11-02T08:23:36.998304Z","shell.execute_reply":"2025-11-02T08:23:36.998320Z"}},"outputs":[],"execution_count":null},{"id":"56f200c0-2464-4439-8268-0ed5ddac16e2","cell_type":"markdown","source":"## 10) Curves (loss / top-1 / top-5)","metadata":{}},{"id":"x5vogvi7Xje2","cell_type":"code","source":"# Cell 10 — Curves\ndef plot_series(names):\n    plt.figure(figsize=(8,5))\n    for n in names:\n        h = load_hist(n)\n        plt.plot(h[\"epoch\"], h[\"top1\"], label=f\"{n}-top1\")\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"Top-1 %\"); plt.grid(True); plt.legend(); plt.show()\n\n    plt.figure(figsize=(8,5))\n    for n in names:\n        h = load_hist(n)\n        plt.plot(h[\"epoch\"], h[\"val_loss\"], label=f\"{n}-loss\")\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.grid(True); plt.legend(); plt.show()\n\nplot_series([\"S_VGG11_SI\",\"S_VGG11_LM_T16\",\"S_VGG11_DKD_T16\",\"S_VGG11_HINTS_T16\",\"S_VGG11_CRD_T16\"])\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5vogvi7Xje2","outputId":"dff3c5d7-3299-45a7-9456-d05ede40115e","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:23:36.999994Z","iopub.status.idle":"2025-11-02T08:23:37.000618Z","shell.execute_reply.started":"2025-11-02T08:23:37.000436Z","shell.execute_reply":"2025-11-02T08:23:37.000466Z"}},"outputs":[],"execution_count":null},{"id":"835f1f93-542b-471a-aeea-7d23e0f58779","cell_type":"markdown","source":"## 11) Probability-distribution alignment (KL/JS) — T vs SI/SD*","metadata":{}},{"id":"VZsZ7cr121jO","cell_type":"code","source":"# Cell 11 — Probability distribution divergences\n@torch.no_grad()\ndef dist_div(teacher, students: dict, loader, T=4.0, metric=\"kl\", batches=20, device=CFG[\"device\"]):\n    teacher.eval(); [m.eval() for m in students.values()]\n    vals={k:[] for k in students}\n    c=0\n    for x,_ in loader:\n        x=x.to(device)\n        pt = F.softmax(teacher(x)/T, dim=1)\n        for name, s in students.items():\n            ps = F.softmax(s(x)/T, dim=1)\n            if metric==\"kl\":\n                v = F.kl_div(ps.log(), pt, reduction='batchmean').item()\n            else:\n                m = 0.5*(pt+ps)\n                v = 0.5*F.kl_div(ps.log(), m, reduction='batchmean').item() + 0.5*F.kl_div(pt.log(), m, reduction='batchmean').item()\n            vals[name].append(v)\n        c+=1\n        if c>=batches: break\n    return {k: float(np.mean(v)) for k,v in vals.items()}\n\n# Build models from best checkpoints\ndef load_student(tag):\n    m = build_vgg11_student().to(CFG[\"device\"])\n    m.load_state_dict(torch.load(os.path.join(CFG[\"save_dir\"], f\"{tag}_best.pt\"), map_location=CFG[\"device\"]))\n    return m.eval()\n\nT16_eval = build_vgg_teacher('vgg16_bn',pretrained=False).to(CFG[\"device\"])\nT16_eval.load_state_dict(torch.load(os.path.join(CFG[\"save_dir\"], \"T16_finetune_best.pt\"), map_location=CFG[\"device\"])); T16_eval.eval()\n\nstudents = {\n    \"SI\": load_student(\"S_VGG11_SI\"),\n    \"LM\": load_student(\"S_VGG11_LM_T16\"),\n    \"DKD\": load_student(\"S_VGG11_DKD_T16\"),\n    \"HINTS\": load_student(\"S_VGG11_HINTS_T16\"),\n    \"CRD\": load_student(\"S_VGG11_CRD_T16\"),\n}\nprint(\"KL(T16 || S*):\", dist_div(T16_eval, students, val_loader, metric=\"kl\"))\nprint(\"JS(T16 || S*):\", dist_div(T16_eval, students, val_loader, metric=\"js\"))\n","metadata":{"id":"VZsZ7cr121jO","outputId":"4d16b6d4-feca-4066-ffd3-7986b09fae07","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:23:37.001955Z","iopub.status.idle":"2025-11-02T08:23:37.002308Z","shell.execute_reply.started":"2025-11-02T08:23:37.002105Z","shell.execute_reply":"2025-11-02T08:23:37.002120Z"}},"outputs":[],"execution_count":null},{"id":"336ab6d3-d276-4e09-a534-be74ee2dafbe","cell_type":"markdown","source":"## 12) Grad-CAM + similarity to teacher","metadata":{}},{"id":"gOC0XjDXCHD-","cell_type":"code","source":"# Cell 12 — Grad-CAM similarity\nclass GradCAM:\n    def __init__(self, model, layer=\"features.42\"):\n        self.m=model.eval(); self.a=None; self.g=None\n        mod=dict([*model.named_modules()])[layer]\n        mod.register_forward_hook(lambda m,i,o: setattr(self,'a',o.detach()))\n        mod.register_full_backward_hook(lambda m,gi,go: setattr(self,'g',go[0].detach()))\n    def __call__(self,x, idx=None):\n        self.m.zero_grad(set_to_none=True)\n        out=self.m(x)\n        if idx is None: idx = out.argmax(1)\n        out[torch.arange(out.size(0)), idx].sum().backward()\n        w=self.g.mean((2,3), keepdim=True)\n        cam=(w*self.a).sum(1,keepdim=True); cam=F.relu(cam)\n        cam=cam/(cam.amax((2,3),keepdim=True).clamp_min(1e-6))\n        return cam\n\ndef cam_cos(camA, camB):\n    A=camA.flatten(1); B=camB.flatten(1)\n    return F.cosine_similarity(F.normalize(A,1), F.normalize(B,1), dim=1).mean().item()\n\n# Build cams\ncam_T = GradCAM(T16_eval, layer=\"features.42\")  # adjust if needed\ncams_S = {k: GradCAM(v, layer=\"features.42\") for k,v in students.items()}\n\n# sample a small batch\nxb,_ = next(iter(val_loader))\nxb = xb.to(CFG[\"device\"])[:32]\ncam_t = cam_T(xb)\nfor name, cammer in cams_S.items():\n    cam_s = cammer(xb)\n    print(name, \"CAM cosine vs Teacher:\", cam_cos(cam_t, cam_s))\n","metadata":{"id":"gOC0XjDXCHD-","outputId":"1a0295be-498a-4924-882f-4104ec1cfae4","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:23:37.004555Z","iopub.status.idle":"2025-11-02T08:23:37.004914Z","shell.execute_reply.started":"2025-11-02T08:23:37.004727Z","shell.execute_reply":"2025-11-02T08:23:37.004744Z"}},"outputs":[],"execution_count":null},{"id":"34304567-f6ca-4006-854a-199bc04e9b9a","cell_type":"markdown","source":"## 13) Color-invariance eval (jittered validation)","metadata":{}},{"id":"x3L4_stFB-dP","cell_type":"code","source":"# Cell 13 — Color invariance evaluation\nval_jitter = transforms.Compose([transforms.ColorJitter(0.4,0.4,0.4,0.1), transforms.ToTensor(), transforms.Normalize(CIFAR100_MEAN,CIFAR100_STD)])\n_, val_loader_j = get_loaders(tfs(True), val_jitter)\n\nS_CRD_col = load_student(\"S_VGG11_CRD_T16Color\")\nS_SI = load_student(\"S_VGG11_SI\")\ndef eval_on(loader, model): return evaluate(model, loader, CFG[\"device\"])[\"top1\"]\nprint(\"Top-1 on jittered val — SI:\", eval_on(val_loader_j, S_SI))\nprint(\"Top-1 on jittered val — CRD(Color teacher):\", eval_on(val_loader_j, S_CRD_col))\n","metadata":{"id":"x3L4_stFB-dP","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:23:37.005722Z","iopub.status.idle":"2025-11-02T08:23:37.006072Z","shell.execute_reply.started":"2025-11-02T08:23:37.005895Z","shell.execute_reply":"2025-11-02T08:23:37.005911Z"}},"outputs":[],"execution_count":null},{"id":"bb1d1b24-8682-49bd-91e5-628f4b65c496","cell_type":"markdown","source":"## 14) Confusion matrix + per-class + reliability/ECE (optional but nice in write-up)","metadata":{}},{"id":"d4ecab8d-7c99-4edb-bbc6-715fca826feb","cell_type":"code","source":"# Cell 14 — (Optional) CM / per-class / ECE\nfrom sklearn.metrics import confusion_matrix\n@torch.no_grad()\ndef preds_targets(m, loader):\n    m.eval(); P=[]; Y=[]\n    for x,y in loader:\n        x=x.to(CFG[\"device\"]); logits=m(x)\n        P.append(logits.argmax(1).cpu().numpy()); Y.append(y.numpy())\n    return np.concatenate(P), np.concatenate(Y)\n\np,y = preds_targets(load_student(\"S_VGG11_LM_T16\"), val_loader)\ncm = confusion_matrix(y,p); cls_acc = (cm.diagonal()/cm.sum(axis=1))\nprint(\"Mean per-class acc:\", cls_acc.mean()*100)\n# Reliability: compute confidences vs accuracy (ECE) if you want.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:23:37.007251Z","iopub.status.idle":"2025-11-02T08:23:37.007583Z","shell.execute_reply.started":"2025-11-02T08:23:37.007399Z","shell.execute_reply":"2025-11-02T08:23:37.007413Z"}},"outputs":[],"execution_count":null}]}