{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Imports and seeds"
      ],
      "metadata": {
        "id": "vRuRxlNuhD4I"
      },
      "id": "vRuRxlNuhD4I"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dabe0fc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dabe0fc3",
        "outputId": "a13b2cb9-3ad3-47a2-df7f-9bf19396333c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True | Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os, random, json, math, time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Device & AMP (new API; no deprecation warnings) ---\n",
        "HAS_CUDA = torch.cuda.is_available()\n",
        "DEVICE   = torch.device('cuda' if HAS_CUDA else 'cpu')\n",
        "from torch.amp import GradScaler, autocast\n",
        "SCALER = GradScaler('cuda') if HAS_CUDA else None\n",
        "def amp_on():  # use: with amp_on(): ...\n",
        "    return autocast('cuda', enabled=HAS_CUDA)\n",
        "\n",
        "# --- CuDNN knobs (throughput > determinism) ---\n",
        "torch.backends.cudnn.benchmark = True   # speed up convs with autotune\n",
        "# If you need strict reproducibility later, set benchmark=False and deterministic flags.\n",
        "\n",
        "# --- Paths ---\n",
        "ROOT  = Path('/content/ATML_A3')\n",
        "CKPTS = ROOT / 'ckpts'\n",
        "RES   = ROOT / 'results'\n",
        "FIGS  = ROOT / 'figures'\n",
        "for p in [CKPTS, RES, FIGS]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Seeding ---\n",
        "def set_seed(seed=1337):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if HAS_CUDA:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1337)\n",
        "print(f\"CUDA available: {HAS_CUDA} | Device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Data: CIFAR-100"
      ],
      "metadata": {
        "id": "M_b2qlJxhYof"
      },
      "id": "M_b2qlJxhYof"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cifar100(batch_size=128, num_workers=2):\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std  = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    train_ds = datasets.CIFAR100(\n",
        "        root=str(ROOT / \"data\"),\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=train_tf\n",
        "    )\n",
        "\n",
        "    val_ds = datasets.CIFAR100(\n",
        "        root=str(ROOT / \"data\"),\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=test_tf\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        drop_last=True   # helps with BatchNorm stability\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_loader, val_loader = get_cifar100()\n",
        "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRITyu4uhVj5",
        "outputId": "67ca0c83-e835-4452-ad60-779c7e059d3b"
      },
      "id": "XRITyu4uhVj5",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 390 | Val batches: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Models: VGG-16/19 (teacher), VGG-11 (student)"
      ],
      "metadata": {
        "id": "R3LwB2WJhimR"
      },
      "id": "R3LwB2WJhimR"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_vgg(name='vgg16', num_classes=100, pretrained=False):\n",
        "    if name == 'vgg16':\n",
        "        net = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    elif name == 'vgg19':\n",
        "        net = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    elif name == 'vgg11':\n",
        "        net = models.vgg11(weights=None)  # student always scratch\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported VGG: {name}\")\n",
        "\n",
        "    in_feats = net.classifier[-1].in_features\n",
        "    net.classifier[-1] = nn.Linear(in_feats, num_classes)\n",
        "\n",
        "    # safer init (VGG uses small std for FC)\n",
        "    nn.init.normal_(net.classifier[-1].weight, mean=0.0, std=0.01)\n",
        "    nn.init.zeros_(net.classifier[-1].bias)\n",
        "\n",
        "\n",
        "    return net.to(DEVICE)\n",
        "\n",
        "def new_teacher():\n",
        "    # ImageNet-pretrained VGG-16; fine-tune with CE; later freeze for KD\n",
        "    t = make_vgg('vgg16', pretrained=True)\n",
        "    return t\n",
        "\n",
        "def new_student():\n",
        "    # VGG-11 from scratch (no pretrained ever)\n",
        "    s = make_vgg('vgg11', pretrained=False)\n",
        "    return s\n",
        "\n",
        "def freeze_teacher(model):\n",
        "    model.eval()\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    return model\n",
        "\n",
        "# Build once here if you want, or build inside run blocks\n",
        "teacher16 = new_teacher()\n",
        "student11 = new_student()\n",
        "print(\"Built VGG-16 (teacher, pretrained) & VGG-11 (student, scratch).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOZRQfYuheQp",
        "outputId": "582aac00-fd53-4482-f619-30159fc501a7"
      },
      "id": "HOZRQfYuheQp",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built VGG-16 (teacher, pretrained) & VGG-11 (student, scratch).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Losses: CE, Label Smoothing, KD-LM, DKD"
      ],
      "metadata": {
        "id": "cJdN5OvphuUz"
      },
      "id": "cJdN5OvphuUz"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Losses ---\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps=0.1):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.logsoft = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, logits, targets):\n",
        "        n = logits.size(1)\n",
        "        logp = self.logsoft(logits)\n",
        "        with torch.no_grad():\n",
        "            dist = torch.zeros_like(logp)\n",
        "            dist.fill_(self.eps / (n - 1))\n",
        "            dist.scatter_(1, targets.unsqueeze(1), 1 - self.eps)\n",
        "        return torch.mean(torch.sum(-dist * logp, dim=1))\n",
        "\n",
        "\n",
        "def kd_loss_logits(student_logits, teacher_logits, T=4.0, eps=1e-8):\n",
        "    # KL( teacher || student ) at temperature T\n",
        "    log_p = F.log_softmax(student_logits / T, dim=1)\n",
        "    q     = F.softmax(teacher_logits / T, dim=1)\n",
        "    q     = q.clamp(min=eps)  # numeric safety\n",
        "    return F.kl_div(log_p, q, reduction='batchmean') * (T * T)\n",
        "\n",
        "\n",
        "class DKDLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoupled KD (Zhang et al.): TCKD + NCKD\n",
        "      - TCKD: KL between [p(y), 1-p(y)] and [q(y), 1-q(y)]\n",
        "      - NCKD: KL over non-target classes, renormalized\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=1.0, beta=8.0, T=4.0, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta  = beta\n",
        "        self.T     = T\n",
        "        self.eps   = eps\n",
        "\n",
        "    def forward(self, s_logits, t_logits, targets):\n",
        "        T, eps = self.T, self.eps\n",
        "\n",
        "        # Full softmax distributions\n",
        "        p_s = F.softmax(s_logits / T, dim=1)\n",
        "        q_t = F.softmax(t_logits / T, dim=1)\n",
        "\n",
        "        # ----- TCKD (2-class distributions: target vs. non-target) -----\n",
        "        idx = targets.unsqueeze(1)                                # [B,1]\n",
        "        p_y = p_s.gather(1, idx)                                  # [B,1]\n",
        "        q_y = q_t.gather(1, idx)                                  # [B,1]\n",
        "\n",
        "        p2 = torch.cat([p_y, 1.0 - p_y], dim=1).clamp(min=eps)    # [B,2]\n",
        "        q2 = torch.cat([q_y, 1.0 - q_y], dim=1).clamp(min=eps)    # [B,2]\n",
        "        tckd = F.kl_div(p2.log(), q2, reduction='batchmean')      # KL(p||q) over 2-way\n",
        "\n",
        "        # ----- NCKD (normalize over non-target classes only) -----\n",
        "        mask = torch.ones_like(p_s).scatter(1, idx, 0.0)          # zero-out target\n",
        "        p_nt = (p_s * mask); q_nt = (q_t * mask)\n",
        "        p_nt = p_nt / (p_nt.sum(dim=1, keepdim=True) + eps)       # renorm\n",
        "        q_nt = q_nt / (q_nt.sum(dim=1, keepdim=True) + eps)\n",
        "        nckd = F.kl_div((p_nt + eps).log(), q_nt, reduction='batchmean')\n",
        "\n",
        "        return (self.alpha * tckd + self.beta * nckd) * (T * T)\n"
      ],
      "metadata": {
        "id": "8YJJMf7xhuc3"
      },
      "id": "8YJJMf7xhuc3",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Eval + generic CE training loop"
      ],
      "metadata": {
        "id": "82w_thi5h2Hd"
      },
      "id": "82w_thi5h2Hd"
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_topk(logits, targets, topk=(1,)):\n",
        "    maxk = max(topk); b = targets.size(0)\n",
        "    _, pred = logits.topk(maxk, 1, True, True); pred = pred.t()\n",
        "    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
        "    out = []\n",
        "    for k in topk:\n",
        "        out.append(correct[:k].reshape(-1).float().sum(0, keepdim=True).mul_(100.0 / b))\n",
        "    return out\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    loss_sum = 0.0; n = 0; top1 = 0.0; top5 = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)\n",
        "            logits = model(x)\n",
        "            loss = ce(logits, y)\n",
        "            a1, a5 = accuracy_topk(logits, y, topk=(1, 5))\n",
        "            bs = x.size(0)\n",
        "            loss_sum += loss.item() * bs; n += bs\n",
        "            top1 += a1.item() * bs / 100.0; top5 += a5.item() * bs / 100.0\n",
        "    return loss_sum / n, 100 * top1 / n, 100 * top5 / n\n",
        "\n",
        "def train_ce(model, train_loader, val_loader, epochs=60, lr=0.1, weight_decay=5e-4, use_ls=False, ls_eps=0.1, clip=1.0):\n",
        "    model.to(DEVICE)\n",
        "    opt  = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "    ce   = LabelSmoothingCE(ls_eps) if use_ls else nn.CrossEntropyLoss()\n",
        "    best = (1e9, 0, 0)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        for x,y in train_loader:\n",
        "            x = x.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            # NO AMP: keep it simple/stable\n",
        "            logits = model(x)\n",
        "            loss = ce(logits, y)\n",
        "\n",
        "            if not torch.isfinite(loss):\n",
        "                # skip toxic batch\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            if clip is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "            opt.step()\n",
        "\n",
        "        sched.step()\n",
        "        vl, a1, a5 = evaluate(model, val_loader)\n",
        "        if a1 > best[1]: best = (vl, a1, a5)\n",
        "        if ep % 10 == 0 or ep == 1:\n",
        "            print(f\"[CE-safe{'-LS' if use_ls else ''}] {ep}/{epochs} | val_loss={vl:.3f} top1={a1:.2f} top5={a5:.2f}\")\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "R6rAcLNbhyok"
      },
      "id": "R6rAcLNbhyok",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) KD-LM training loop"
      ],
      "metadata": {
        "id": "107Y6vFTh555"
      },
      "id": "107Y6vFTh555"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kd_lm(student, teacher, train_loader, val_loader,\n",
        "                epochs=60, lr=0.1, alpha=0.6, T=3.5,\n",
        "                warmup_epochs=5, clip=1.0):\n",
        "    \"\"\"\n",
        "    Stable Logit Matching:\n",
        "      - teacher: eval + frozen (no grads)\n",
        "      - student: grads ON\n",
        "      - CE warm-up for first `warmup_epochs`, then mix in KD with weight `alpha`\n",
        "      - No AMP for KD (avoids overflow); optional grad clipping\n",
        "    \"\"\"\n",
        "    # --- freeze teacher, enable student ---\n",
        "    teacher.eval().to(DEVICE)\n",
        "    for p in teacher.parameters(): p.requires_grad = False\n",
        "    student.to(DEVICE)\n",
        "    for p in student.parameters(): p.requires_grad = True\n",
        "\n",
        "    ce   = nn.CrossEntropyLoss()\n",
        "    opt  = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "\n",
        "    best = (1e9, 0, 0)\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        student.train()\n",
        "        use_kd = ep > warmup_epochs  # CE-only warm-up\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            with torch.no_grad():                 # teacher forward has NO grads\n",
        "                t_logits = teacher(x)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            # IMPORTANT: no AMP here for stability\n",
        "            s_logits = student(x)\n",
        "            loss = ce(s_logits, y)\n",
        "            if use_kd and alpha > 0.0:\n",
        "                kd = kd_loss_logits(s_logits, t_logits, T=T)\n",
        "                if torch.isfinite(kd):\n",
        "                    loss = (1 - alpha) * loss + alpha * kd\n",
        "\n",
        "            if not torch.isfinite(loss):\n",
        "                continue  # skip pathological batch\n",
        "\n",
        "            loss.backward()\n",
        "            if clip is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=clip)\n",
        "            opt.step()\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        vl, a1, a5 = evaluate(student, val_loader)\n",
        "        if a1 > best[1]: best = (vl, a1, a5)\n",
        "        if ep % 10 == 0 or ep == 1:\n",
        "            print(f\"[KD-LM (safe)] {ep}/{epochs} | use_kd={use_kd} alpha={alpha:.2f} T={T} \"\n",
        "                  f\"| val_loss={vl:.3f} top1={a1:.2f} top5={a5:.2f}\")\n",
        "\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "j-cnF35Lh5Vt"
      },
      "id": "j-cnF35Lh5Vt",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) DKD training loop"
      ],
      "metadata": {
        "id": "dTePTUiBh-Tn"
      },
      "id": "dTePTUiBh-Tn"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dkd(student, teacher, train_loader, val_loader,\n",
        "              epochs=60, lr=0.1, alpha=1.0, beta=8.0, T=3.5,\n",
        "              warmup_epochs=5, clip=1.0):\n",
        "    \"\"\"\n",
        "    Stable DKD:\n",
        "      - Teacher frozen in eval\n",
        "      - CE-only warm-up, then add DKD\n",
        "      - No AMP (KD is numerically touchy)\n",
        "      - Grad clipping + finite-loss guard\n",
        "    \"\"\"\n",
        "    # freeze teacher, enable student\n",
        "    teacher.eval().to(DEVICE)\n",
        "    for p in teacher.parameters(): p.requires_grad = False\n",
        "    student.to(DEVICE)\n",
        "    for p in student.parameters(): p.requires_grad = True\n",
        "\n",
        "    dkd = DKDLoss(alpha=alpha, beta=beta, T=T)\n",
        "    ce  = nn.CrossEntropyLoss()\n",
        "    opt = torch.optim.SGD(student.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "\n",
        "    best = (1e9, 0, 0)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        student.train()\n",
        "        use_dkd = ep > warmup_epochs  # CE warm-up\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)\n",
        "            with torch.no_grad():\n",
        "                t_logits = teacher(x)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            # NO AMP for KD (stability first)\n",
        "            s_logits = student(x)\n",
        "            loss = ce(s_logits, y)\n",
        "            if use_dkd:\n",
        "                dk = dkd(s_logits, t_logits, y)\n",
        "                if torch.isfinite(dk):\n",
        "                    loss = loss + dk\n",
        "\n",
        "            if not torch.isfinite(loss):\n",
        "                continue  # skip pathological batch\n",
        "\n",
        "            loss.backward()\n",
        "            if clip is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=clip)\n",
        "            opt.step()\n",
        "\n",
        "        sched.step()\n",
        "        vl, a1, a5 = evaluate(student, val_loader)\n",
        "        if a1 > best[1]: best = (vl, a1, a5)\n",
        "        if ep % 10 == 0 or ep == 1:\n",
        "            print(f\"[DKD (safe)] {ep}/{epochs} | use_dkd={use_dkd} T={T} \"\n",
        "                  f\"| val_loss={vl:.3f} top1={a1:.2f} top5={a5:.2f}\")\n",
        "\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "W2xrnpKAh-pS"
      },
      "id": "W2xrnpKAh-pS",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) Save/Load helpers"
      ],
      "metadata": {
        "id": "cy7PZBhsl-Rg"
      },
      "id": "cy7PZBhsl-Rg"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_ckpt(model, path):\n",
        "    torch.save({'state_dict': model.state_dict()}, path)\n",
        "    print(f\"✅ Saved checkpoint to: {path}\")\n",
        "\n",
        "def load_ckpt(model, path, strict=True):\n",
        "    ckpt = torch.load(path, map_location=DEVICE)\n",
        "    model.load_state_dict(ckpt['state_dict'], strict=strict)\n",
        "    print(f\"✅ Loaded checkpoint from: {path}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "EzmXYuVsl-aL"
      },
      "id": "EzmXYuVsl-aL",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) Orchestrate Task 3.1 (Teacher → SI → LS → LM → DKD)"
      ],
      "metadata": {
        "id": "qQdqOBwLmHcn"
      },
      "id": "qQdqOBwLmHcn"
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 60  # use 30-40 for a fast pass if needed\n",
        "\n",
        "RUN_TRAIN_TEACHER = True\n",
        "RUN_TRAIN_SI      = True\n",
        "RUN_TRAIN_LS      = True\n",
        "RUN_TRAIN_LM      = True\n",
        "RUN_TRAIN_DKD     = True\n",
        "\n",
        "# ---------- 1) Teacher (VGG-16, pretrained ImageNet) ----------\n",
        "\n",
        "teacher_ckpt = CKPTS/'teacher_vgg16.pt'\n",
        "teacher16 = new_teacher()  # builds with safe init now\n",
        "best_T = train_ce(teacher16, train_loader, val_loader, epochs=60, lr=0.01)  # << lower LR\n",
        "save_ckpt(teacher16, CKPTS/'teacher_vgg16.pt')\n",
        "print(\"Teacher saved:\", best_T)\n",
        "\n",
        "# Freeze teacher for KD\n",
        "teacher16.eval()\n",
        "for p in teacher16.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# ---------- 2) Student Independent (VGG-11, CE only) ----------\n",
        "si_ckpt = CKPTS/'student_SI.pt'\n",
        "if si_ckpt.exists() and not RUN_TRAIN_SI:\n",
        "    si = make_vgg('vgg11', pretrained=False)\n",
        "    load_ckpt(si, si_ckpt)\n",
        "    print(\"Loaded SI from ckpt.\")\n",
        "else:\n",
        "    si = make_vgg('vgg11', pretrained=False)\n",
        "    if RUN_TRAIN_SI:\n",
        "        best_SI = train_ce(si, train_loader, val_loader, epochs=EPOCHS, lr=0.1, tag=\"SI\")\n",
        "        save_ckpt(si, si_ckpt)\n",
        "        print(\"SI saved:\", best_SI)\n",
        "\n",
        "# ---------- 3) Label Smoothing baseline (VGG-11) ----------\n",
        "ls_ckpt = CKPTS/'student_LS.pt'\n",
        "if ls_ckpt.exists() and not RUN_TRAIN_LS:\n",
        "    ls = make_vgg('vgg11', pretrained=False)\n",
        "    load_ckpt(ls, ls_ckpt)\n",
        "    print(\"Loaded LS from ckpt.\")\n",
        "else:\n",
        "    ls = make_vgg('vgg11', pretrained=False)\n",
        "    if RUN_TRAIN_LS:\n",
        "        best_LS = train_ce(ls, train_loader, val_loader, epochs=EPOCHS, lr=0.1, use_ls=True, ls_eps=0.1, tag=\"LS\")\n",
        "        save_ckpt(ls, ls_ckpt)\n",
        "        print(\"LS saved:\", best_LS)\n",
        "\n",
        "# ---------- 4) KD — Logit Matching (VGG-11 distilled from VGG-16) ----------\n",
        "lm_ckpt = CKPTS/'student_LM.pt'\n",
        "if lm_ckpt.exists() and not RUN_TRAIN_LM:\n",
        "    lm = make_vgg('vgg11', pretrained=False)\n",
        "    load_ckpt(lm, lm_ckpt)\n",
        "    print(\"Loaded LM from ckpt.\")\n",
        "else:\n",
        "    lm = make_vgg('vgg11', pretrained=False)\n",
        "    if RUN_TRAIN_LM:\n",
        "        # SAFE KD settings (no AMP, CE warm-up, clip)\n",
        "        best_LM = train_kd_lm(\n",
        "            lm, teacher16, train_loader, val_loader,\n",
        "            epochs=EPOCHS, lr=0.05, alpha=0.7, T=3.5,\n",
        "            warmup_epochs=5, clip=1.0\n",
        "        )\n",
        "        save_ckpt(lm, lm_ckpt)\n",
        "        print(\"LM saved:\", best_LM)\n",
        "\n",
        "# ---------- 5) KD — Decoupled KD (VGG-11 distilled from VGG-16) ----------\n",
        "dkd_ckpt = CKPTS/'student_DKD.pt'\n",
        "if dkd_ckpt.exists() and not RUN_TRAIN_DKD:\n",
        "    dkd = make_vgg('vgg11', pretrained=False)\n",
        "    load_ckpt(dkd, dkd_ckpt)\n",
        "    print(\"Loaded DKD from ckpt.\")\n",
        "else:\n",
        "    dkd = make_vgg('vgg11', pretrained=False)\n",
        "    if RUN_TRAIN_DKD:\n",
        "        # SAFE DKD settings (no AMP, CE warm-up, clip)\n",
        "        best_DKD = train_dkd(\n",
        "            dkd, teacher16, train_loader, val_loader,\n",
        "            epochs=EPOCHS, lr=0.05, alpha=1.0, beta=8.0, T=3.5,\n",
        "            warmup_epochs=5, clip=1.0\n",
        "        )\n",
        "        save_ckpt(dkd, dkd_ckpt)\n",
        "        print(\"DKD saved:\", best_DKD)\n",
        "\n",
        "# ---------- Summary ----------\n",
        "def quick_eval(tag, path, arch):\n",
        "    m = make_vgg(arch, pretrained=False)\n",
        "    load_ckpt(m, path)\n",
        "    vl, a1, a5 = evaluate(m, val_loader)\n",
        "    return {\"method\": tag, \"top1\": a1, \"top5\": a5, \"val_loss\": vl}\n",
        "\n",
        "summary = []\n",
        "if teacher_ckpt.exists(): summary.append(quick_eval(\"Teacher (VGG16)\", teacher_ckpt, 'vgg16'))\n",
        "if si_ckpt.exists():      summary.append(quick_eval(\"SI (VGG11)\",      si_ckpt,      'vgg11'))\n",
        "if ls_ckpt.exists():      summary.append(quick_eval(\"LS (VGG11)\",      ls_ckpt,      'vgg11'))\n",
        "if lm_ckpt.exists():      summary.append(quick_eval(\"LM (KD)\",         lm_ckpt,      'vgg11'))\n",
        "if dkd_ckpt.exists():     summary.append(quick_eval(\"DKD\",             dkd_ckpt,     'vgg11'))\n",
        "\n",
        "df = pd.DataFrame(summary).sort_values(\"top1\", ascending=False)\n",
        "print(df.to_string(index=False))\n",
        "df.to_csv(RES/'task3_summary.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Ob0ZNXiLmDkM",
        "outputId": "5bd57dd2-d38e-407d-b630-434d733b409a"
      },
      "id": "Ob0ZNXiLmDkM",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CE-safe] 1/60 | val_loss=2.143 top1=41.48 top5=75.90\n",
            "[CE-safe] 10/60 | val_loss=1.287 top1=63.36 top5=89.58\n",
            "[CE-safe] 20/60 | val_loss=1.210 top1=66.59 top5=91.00\n",
            "[CE-safe] 30/60 | val_loss=1.158 top1=68.48 top5=91.52\n",
            "[CE-safe] 40/60 | val_loss=1.188 top1=68.48 top5=91.35\n",
            "[CE-safe] 50/60 | val_loss=1.188 top1=69.13 top5=91.71\n",
            "[CE-safe] 60/60 | val_loss=1.190 top1=69.34 top5=91.66\n",
            "✅ Saved checkpoint to: /content/ATML_A3/ckpts/teacher_vgg16.pt\n",
            "Teacher saved: (1.1941225917816163, 69.44, 91.64)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "train_ce() got an unexpected keyword argument 'tag'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2070755710.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0msi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg11'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mRUN_TRAIN_SI\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mbest_SI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0msave_ckpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msi_ckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SI saved:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_SI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: train_ce() got an unexpected keyword argument 'tag'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "si_ckpt = CKPTS/'student_SI.pt'\n",
        "si = make_vgg('vgg11', pretrained=False)\n",
        "best_SI = train_ce(si, train_loader, val_loader, epochs=EPOCHS, lr=0.1)\n",
        "save_ckpt(si, si_ckpt)\n",
        "print(\"SI saved:\", best_SI)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5vogvi7Xje2",
        "outputId": "dff3c5d7-3299-45a7-9456-d05ede40115e"
      },
      "id": "x5vogvi7Xje2",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CE-safe] 1/60 | val_loss=4.292 top1=2.72 top5=13.34\n",
            "[CE-safe] 10/60 | val_loss=2.640 top1=30.56 top5=63.77\n",
            "[CE-safe] 20/60 | val_loss=2.010 top1=46.86 top5=76.10\n",
            "[CE-safe] 30/60 | val_loss=1.730 top1=52.95 top5=81.25\n",
            "[CE-safe] 40/60 | val_loss=1.569 top1=58.50 top5=84.84\n",
            "[CE-safe] 50/60 | val_loss=1.486 top1=62.34 top5=86.45\n",
            "[CE-safe] 60/60 | val_loss=1.534 top1=62.59 top5=86.84\n",
            "✅ Saved checkpoint to: /content/ATML_A3/ckpts/student_SI.pt\n",
            "SI saved: (1.5340964878082275, 62.72, 86.87)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls_ckpt = CKPTS/'student_LS.pt'\n",
        "ls = make_vgg('vgg11', pretrained=False)\n",
        "best_LS = train_ce(ls, train_loader, val_loader, epochs=EPOCHS, lr=0.1, use_ls=True, ls_eps=0.1)\n",
        "save_ckpt(ls, ls_ckpt)\n",
        "print(\"LS saved:\", best_LS)"
      ],
      "metadata": {
        "id": "VZsZ7cr121jO",
        "outputId": "4d16b6d4-feca-4066-ffd3-7986b09fae07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VZsZ7cr121jO",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CE-safe-LS] 1/60 | val_loss=4.274 top1=3.18 top5=14.93\n",
            "[CE-safe-LS] 10/60 | val_loss=2.574 top1=35.05 top5=66.89\n",
            "[CE-safe-LS] 20/60 | val_loss=2.092 top1=47.43 top5=76.92\n",
            "[CE-safe-LS] 30/60 | val_loss=1.691 top1=55.31 top5=82.99\n",
            "[CE-safe-LS] 40/60 | val_loss=1.516 top1=60.27 top5=85.80\n",
            "[CE-safe-LS] 50/60 | val_loss=1.401 top1=63.44 top5=87.07\n",
            "[CE-safe-LS] 60/60 | val_loss=1.395 top1=63.98 top5=87.06\n",
            "✅ Saved checkpoint to: /content/ATML_A3/ckpts/student_LS.pt\n",
            "LS saved: (1.3961281625747681, 64.09, 87.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ofz3m3u4XyVF"
      },
      "id": "ofz3m3u4XyVF"
    },
    {
      "cell_type": "code",
      "source": [
        "lm_ckpt = CKPTS/'student_LM.pt'\n",
        "lm = make_vgg('vgg11', pretrained=False)\n",
        "best_LM = train_kd_lm(\n",
        "    lm, teacher16, train_loader, val_loader,\n",
        "    epochs=60, lr=0.07, alpha=0.8, T=3.0,\n",
        "    warmup_epochs=5, clip=1.0\n",
        ")\n",
        "print(\"LM saved:\", best_LM)"
      ],
      "metadata": {
        "id": "gOC0XjDXCHD-",
        "outputId": "1a0295be-498a-4924-882f-4104ec1cfae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gOC0XjDXCHD-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[KD-LM (safe)] 1/60 | use_kd=False alpha=0.80 T=3.0 | val_loss=4.340 top1=2.41 top5=12.17\n",
            "[KD-LM (safe)] 10/60 | use_kd=True alpha=0.80 T=3.0 | val_loss=3.596 top1=26.17 top5=55.86\n",
            "[KD-LM (safe)] 20/60 | use_kd=True alpha=0.80 T=3.0 | val_loss=3.003 top1=39.65 top5=69.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x3L4_stFB-dP"
      },
      "id": "x3L4_stFB-dP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10) Quick metrics dump (for your report table/plot later)"
      ],
      "metadata": {
        "id": "tsBRJdkdmTPz"
      },
      "id": "tsBRJdkdmTPz"
    },
    {
      "cell_type": "code",
      "source": [
        "def snapshot_metrics(method_name, model):\n",
        "    vl,a1,a5 = evaluate(model, val_loader)\n",
        "    return {'method':method_name, 'top1_acc':a1, 'top5_acc':a5, 'val_loss':vl}\n",
        "\n",
        "rows = []\n",
        "# Reload to be safe (in case you restart cells later)\n",
        "T  = make_vgg('vgg16'); load_ckpt(T,  str(CKPTS/'teacher_vgg16.pt'))\n",
        "SI = make_vgg('vgg11'); load_ckpt(SI, str(CKPTS/'student_SI.pt'))\n",
        "LS = make_vgg('vgg11'); load_ckpt(LS, str(CKPTS/'student_LS.pt'))\n",
        "LM = make_vgg('vgg11'); load_ckpt(LM, str(CKPTS/'student_LM.pt'))\n",
        "DK = make_vgg('vgg11'); load_ckpt(DK, str(CKPTS/'student_DKD.pt'))\n",
        "\n",
        "for name, m in [('SI',SI), ('LS',LS), ('LM',LM), ('DKD',DK)]:\n",
        "    rows.append(snapshot_metrics(name, m))\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(RES/'task3_part1_metrics.csv', index=False)\n",
        "print(df)\n",
        "print(\"Saved:\", RES/'task3_part1_metrics.csv')\n"
      ],
      "metadata": {
        "id": "ztvP4MdPmMn3"
      },
      "id": "ztvP4MdPmMn3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}